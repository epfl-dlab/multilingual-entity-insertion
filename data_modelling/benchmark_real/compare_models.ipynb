{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import find_context from baselines/bm25/bm25.py\n",
    "import random\n",
    "from urllib import parse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(rankings, k_values=[1, 5, 10]):\n",
    "    mrr_at_k = {str(k): {'score': 0, 'cl': [None, None]} for k in k_values}\n",
    "    mrr_at_k_lists = {str(k): [0 for _ in range(100)] for k in k_values}\n",
    "    # compute MRR for each k\n",
    "    # and compute confidence intervals for each k\n",
    "    mrr_at_max = {'score': 0, 'cl': [None, None]}\n",
    "    mrr_at_max_list = [0 for _ in range(100)]\n",
    "    \n",
    "    # extract 100 samples\n",
    "    for i in range(100):\n",
    "        sample = np.array(random.choices(rankings, k = len(rankings)))\n",
    "        for k in mrr_at_k_lists.keys():\n",
    "            # get all the elements in sample where rank <= k\n",
    "            sample_k = sample[sample <= int(k)]\n",
    "            if len(sample_k) > 0:\n",
    "                mrr_at_k_lists[str(k)][i] = np.sum(1 / sample_k) / len(sample)\n",
    "        mrr_at_max_list[i] = np.mean(1 / sample)\n",
    "    \n",
    "    # sort values for confidence intervals\n",
    "    for k in mrr_at_k_lists.keys():\n",
    "        mrr_at_k_lists[str(k)].sort()\n",
    "    mrr_at_max_list.sort()\n",
    "        \n",
    "    # find mean score and 90% confidence interval\n",
    "    for k in mrr_at_k_lists.keys():\n",
    "        mrr_at_k[str(k)]['score'] = np.mean(mrr_at_k_lists[str(k)]) * 1000 // 1 / 1000\n",
    "        mrr_at_k[str(k)]['cl'] = [mrr_at_k_lists[str(k)][4] * 1000 // 1 / 1000, mrr_at_k_lists[str(k)][94] * 1000 // 1 / 1000]\n",
    "    mrr_at_max['score'] = np.mean(mrr_at_max_list) * 1000 // 1 / 1000\n",
    "    mrr_at_max['cl'] = [mrr_at_max_list[4] * 1000 // 1 / 1000, mrr_at_max_list[94] * 1000 // 1 / 1000]\n",
    "    \n",
    "    # add max to mrr_at_k\n",
    "    mrr_at_k['max'] = mrr_at_max\n",
    "\n",
    "    return mrr_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stratified_mrr(rankings, categories, categories_values, k_values=[1, 5, 10]):\n",
    "    ranks_stratified = {category: [] for category in categories_values}\n",
    "    for rank, category in zip(rankings, categories):\n",
    "        ranks_stratified[category].append(rank)\n",
    "        \n",
    "    discrete_mrr_at_k = {}\n",
    "        \n",
    "    for category in categories_values:\n",
    "        discrete_mrr_at_k[category] = compute_mrr(ranks_stratified[category], k_values)\n",
    "        \n",
    "    return discrete_mrr_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hits(rankings, k_values=[1, 5, 10]):\n",
    "    hits_at_k = {str(k): {'score': 0, 'cl': [None, None]} for k in k_values}\n",
    "    hits_at_k_lists = {str(k): [0 for _ in range(100)] for k in k_values}\n",
    "    \n",
    "    # extract 100 samples\n",
    "    for i in range(100):\n",
    "        sample = np.array(random.choices(rankings, k = len(rankings)))\n",
    "        for k in hits_at_k_lists.keys():\n",
    "            hits_at_k_lists[str(k)][i] = np.sum(sample <= int(k)) / len(sample)\n",
    "    \n",
    "    # sort values for confidence intervals\n",
    "    for k in hits_at_k_lists.keys():\n",
    "        hits_at_k_lists[str(k)].sort()\n",
    "    \n",
    "    # find mean score and 90% confidence interval\n",
    "    for k in hits_at_k_lists.keys():\n",
    "        hits_at_k[str(k)]['score'] = np.mean(hits_at_k_lists[str(k)]) * 1000 // 1 / 1000\n",
    "        hits_at_k[str(k)]['cl'] = [hits_at_k_lists[str(k)][4] * 1000 // 1 / 1000, hits_at_k_lists[str(k)][94] * 1000 // 1 / 1000]\n",
    "    \n",
    "    return hits_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stratified_hits(rankings, categories, categories_values, k_values=[1, 5, 10]):\n",
    "    ranks_stratified = {category: [] for category in categories_values}\n",
    "    for rank, category in zip(rankings, categories):\n",
    "        ranks_stratified[category].append(rank)\n",
    "    \n",
    "    discrete_hits_at_k = {}\n",
    "        \n",
    "    for category in categories_values:\n",
    "        discrete_hits_at_k[category] = compute_hits(ranks_stratified[category], k_values)\n",
    "        \n",
    "    return discrete_hits_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg(rankings, k_values=[1, 5, 10]):\n",
    "    ndcg_at_k = {str(k): {'score': 0, 'cl': [None, None]} for k in k_values}\n",
    "    ndcg_at_k_lists = {str(k): [0 for _ in range(100)] for k in k_values}\n",
    "    # compute NDCG for each k\n",
    "    # and compute confidence intervals for each k\n",
    "    ndcg_at_max = {'score': 0, 'cl': [None, None]}\n",
    "    ndcg_at_max_list = [0 for _ in range(100)]\n",
    "    \n",
    "    # extract 100 samples\n",
    "    for i in range(100):\n",
    "        sample = np.array(random.choices(rankings, k = len(rankings)))\n",
    "        for k in ndcg_at_k_lists.keys():\n",
    "            # get all the elements in sample where rank <= k\n",
    "            sample_k = sample[sample <= int(k)]\n",
    "            if len(sample_k) > 0:\n",
    "                ndcg_at_k_lists[str(k)][i] = np.sum(1 / np.log2(sample_k + 1)) / len(sample)\n",
    "        ndcg_at_max_list[i] = np.sum(1 / np.log2(sample + 1)) / len(sample)\n",
    "    \n",
    "    # sort values for confidence intervals\n",
    "    for k in ndcg_at_k_lists.keys():\n",
    "        ndcg_at_k_lists[str(k)].sort()\n",
    "    ndcg_at_max_list.sort()\n",
    "        \n",
    "    # find mean score and 90% confidence interval\n",
    "    for k in ndcg_at_k_lists.keys():\n",
    "        ndcg_at_k[str(k)]['score'] = np.mean(ndcg_at_k_lists[str(k)]) * 1000 // 1 / 1000\n",
    "        ndcg_at_k[str(k)]['cl'] = [ndcg_at_k_lists[str(k)][4] * 1000 // 1 / 1000, ndcg_at_k_lists[str(k)][94] * 1000 // 1 / 1000]\n",
    "    ndcg_at_max['score'] = np.mean(ndcg_at_max_list) * 1000 // 1 / 1000\n",
    "    ndcg_at_max['cl'] = [ndcg_at_max_list[4] * 1000 // 1 / 1000, ndcg_at_max_list[94] * 1000 // 1 / 1000]\n",
    "    \n",
    "    # add max to ndcg_at_k\n",
    "    ndcg_at_k['max'] = ndcg_at_max\n",
    "    \n",
    "    return ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stratified_ndcg(rankings, categories, categories_values, k_values=[1, 5, 10]):\n",
    "    ranks_stratified = {category: [] for category in categories_values}\n",
    "    for rank, category in zip(rankings, categories):\n",
    "        ranks_stratified[category].append(rank)\n",
    "    \n",
    "    discrete_ndcg_at_k = {}\n",
    "        \n",
    "    for category in categories_values:\n",
    "        discrete_ndcg_at_k[category] = compute_ndcg(ranks_stratified[category], k_values)\n",
    "        \n",
    "    return discrete_ndcg_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('test_ranking_scores.parquet')\n",
    "test_df['missing_category'] = test_df['missing_category'].fillna('present')\n",
    "test_df['number_candidates'] = test_df['negative_contexts'].apply(lambda x: len(literal_eval(x)) + 1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_at_k = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        mrr_at_k[column] = compute_mrr(test_df[column].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_mrr_at_k = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        stratified_mrr_at_k[column] = compute_stratified_mrr(test_df[column].tolist(), test_df['missing_category'].tolist(), test_df['missing_category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        hits_at_k[column] = compute_hits(test_df[column].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_hits_at_k = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        stratified_hits_at_k[column] = compute_stratified_hits(test_df[column].tolist(), test_df['missing_category'].tolist(), test_df['missing_category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg_at_k = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        ndcg_at_k[column] = compute_ndcg(test_df[column].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_ndcg_at_k = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        stratified_ndcg_at_k[column] = compute_stratified_ndcg(test_df[column].tolist(), test_df['missing_category'].tolist(), test_df['missing_category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        std[column] = test_df[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_std = {}\n",
    "for column in test_df.columns:\n",
    "    if 'rank' in column:\n",
    "        stratified_std[column] = test_df.groupby('missing_category')[column].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print standard deviation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in std:\n",
    "    print(f\"{column} std: {std[column]}\")\n",
    "    for category in stratified_std[column].keys():\n",
    "        print(f\"\\t{column} std for {category}: {stratified_std[column][category]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib params\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "# set marker size\n",
    "markersize = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot aggregated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['random_rank', 'bm25_mentions_rank', 'fuzzy_match_rank', 'model_ranking_rank', 'model_ranking_corrupt_section_mentions_rank']\n",
    "labels = ['Random', 'BM25', 'String Match', 'Fine-Tune LM', 'Ours']\n",
    "markers = ['o', 'v', 's', 'D', 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ranking scores\n",
    "# create a grid of 1x3 plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "fig.suptitle('Aggregate ranking scores')    \n",
    "for ax in axs:\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('Score')\n",
    "\n",
    "axs[0].set_title('Hits@k')\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[0].errorbar(hits_at_k[column].keys(), [hits_at_k[column][k]['score'] for k in hits_at_k[column].keys()], yerr=[[hits_at_k[column][k]['score'] - hits_at_k[column][k]['cl'][0] for k in hits_at_k[column].keys()], [hits_at_k[column][k]['cl'][1] - hits_at_k[column][k]['score'] for k in hits_at_k[column].keys()]], label=label, marker=marker, capsize=10, markersize=8)\n",
    "\n",
    "axs[1].set_title('MRR@k')\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[1].errorbar(mrr_at_k[column].keys(), [mrr_at_k[column][k]['score'] for k in mrr_at_k[column].keys()], yerr=[[mrr_at_k[column][k]['score'] - mrr_at_k[column][k]['cl'][0] for k in mrr_at_k[column].keys()], [mrr_at_k[column][k]['cl'][1] - mrr_at_k[column][k]['score'] for k in mrr_at_k[column].keys()]], label=label, marker=marker, capsize=10, markersize=8)\n",
    "\n",
    "axs[2].set_title('NDCG@k')\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[2].errorbar(ndcg_at_k[column].keys(), [ndcg_at_k[column][k]['score'] for k in ndcg_at_k[column].keys()], yerr=[[ndcg_at_k[column][k]['score'] - ndcg_at_k[column][k]['cl'][0] for k in ndcg_at_k[column].keys()], [ndcg_at_k[column][k]['cl'][1] - ndcg_at_k[column][k]['score'] for k in ndcg_at_k[column].keys()]], label=label, marker=marker, capsize=10, markersize=8)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Stratified Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MRR stratified\n",
    "# create a grid of 1xC plots, where C is the number of k values\n",
    "# each plot should have the categories on the x axis and the MRR on the y axis\n",
    "\n",
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(30, 10))\n",
    "fig.suptitle('MRR')    \n",
    "for ax in axs:\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('MRR')\n",
    "    ax.set_title(f\"MRR@{list(mrr_at_k[columns[0]].keys())[axs.tolist().index(ax)]}\")\n",
    "    \n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    for i, k in enumerate(mrr_at_k[column].keys()):\n",
    "        axs[i].errorbar(strategies, [stratified_mrr_at_k[column][s][k]['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s][k]['score'] - stratified_mrr_at_k[column][s][k]['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s][k]['cl'][1] - stratified_mrr_at_k[column][s][k]['score'] for s in strategies]], label=label, marker=marker, capsize=10, markersize=8, linestyle='None')\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hits stratified\n",
    "# create a grid of 1xC plots, where C is the number of k values\n",
    "# each plot should have the categories on the x axis and the hits on the y axis\n",
    "\n",
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 10))\n",
    "fig.suptitle('Hits')\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Hits')\n",
    "    ax.set_title(f\"Hits@{list(hits_at_k[columns[0]].keys())[axs.tolist().index(ax)]}\")\n",
    "    \n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    for i, k in enumerate(hits_at_k[column].keys()):\n",
    "        axs[i].errorbar(strategies, [stratified_hits_at_k[column][s][k]['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s][k]['score'] - stratified_hits_at_k[column][s][k]['cl'][0] for s in strategies], [stratified_hits_at_k[column][s][k]['cl'][1] - stratified_hits_at_k[column][s][k]['score'] for s in strategies]], label=label, marker=marker, capsize=10, markersize=8, linestyle='None')\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot NDCG stratified\n",
    "# create a grid of 1xC plots, where C is the number of k values\n",
    "# each plot should have the categories on the x axis and the ndcg on the y axis\n",
    "\n",
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "fig.suptitle('NDCG')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('NDCG')\n",
    "    ax.set_title(f\"NDCG@{list(ndcg_at_k[columns[0]].keys())[axs.tolist().index(ax)]}\")\n",
    "    \n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    for i, k in enumerate(ndcg_at_k[column].keys()):\n",
    "        axs[i].errorbar(strategies, [stratified_ndcg_at_k[column][s][k]['score'] for s in strategies], yerr=[[stratified_ndcg_at_k[column][s][k]['score'] - stratified_ndcg_at_k[column][s][k]['cl'][0] for s in strategies], [stratified_ndcg_at_k[column][s][k]['cl'][1] - stratified_ndcg_at_k[column][s][k]['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None')\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')    \n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of number of candidates with reduced metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['number_candidates'].hist(bins=100)\n",
    "# set log x axis\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of candidates')\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Number of candidates distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_right_limits = [5, 10, 50, 100, 1000]\n",
    "def find_bin(x):\n",
    "    start = 0\n",
    "    for limit in bins_right_limits:\n",
    "        if x <= limit:\n",
    "            return f\"{start}-{limit}\"\n",
    "        start = limit\n",
    "test_df['candidates_binned'] = test_df['number_candidates'].apply(find_bin)\n",
    "test_df['candidates_binned'].value_counts()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_mrr_at_k = {}\n",
    "binned_hits_at_k = {}\n",
    "for bin in test_df['candidates_binned'].unique():\n",
    "    binned_mrr_at_k[bin] = {}\n",
    "    binned_hits_at_k[bin] = {}\n",
    "    for column in test_df.columns:\n",
    "        if 'rank' in column:\n",
    "            binned_mrr_at_k[bin][column] = compute_stratified_mrr(test_df[test_df['candidates_binned'] == bin][column].tolist(), test_df[test_df['candidates_binned'] == bin]['missing_category'].tolist(), test_df[test_df['candidates_binned'] == bin]['missing_category'].unique())\n",
    "            binned_hits_at_k[bin][column] = compute_stratified_hits(test_df[test_df['candidates_binned'] == bin][column].tolist(), test_df[test_df['candidates_binned'] == bin]['missing_category'].tolist(), test_df[test_df['candidates_binned'] == bin]['missing_category'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "bins = ['0-5', '5-10', '10-50', '50-100', '100-1000']\n",
    "columns = ['random_rank', 'bm25_mentions_rank', 'fuzzy_match_rank', 'model_ranking_rank', 'model_ranking_corrupt_section_mentions_rank']\n",
    "labels = ['Random', 'BM25', 'String Match', 'LM Fine-Tuning', 'Ours']\n",
    "markers = ['o', 'v', 's', '^', 'D']\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(40, 12))\n",
    "fig.suptitle('Binned Performance Comparison - MRR')\n",
    "axs[0].set_ylabel('MRR')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Link Insertion Category')\n",
    "    ax.set_title(F'{bins[i]} candidates')\n",
    "    \n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    for i, bin in enumerate(bins):\n",
    "        axs[i].errorbar(strategies, [binned_mrr_at_k[bin][column][s]['max']['score'] for s in strategies], yerr=[[binned_mrr_at_k[bin][column][s]['max']['score'] - binned_mrr_at_k[bin][column][s]['max']['cl'][0] for s in strategies], [binned_mrr_at_k[bin][column][s]['max']['cl'][1] - binned_mrr_at_k[bin][column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "bins = ['0-5', '5-10', '10-50', '50-100', '100-1000']\n",
    "columns = ['random_rank', 'bm25_mentions_rank', 'fuzzy_match_rank', 'model_ranking_rank', 'model_ranking_corrupt_section_mentions_rank']\n",
    "labels = ['Random', 'BM25', 'String Match', 'LM Fine-Tuning', 'Ours']\n",
    "markers = ['o', 'v', 's', '^', 'D']\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(40, 12))\n",
    "fig.suptitle('Binned Performance Comparison - Hits@1')\n",
    "axs[0].set_ylabel('Hits@1')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Link Insertion Category')\n",
    "    ax.set_title(F'{bins[i]} candidates')\n",
    "    \n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    for i, bin in enumerate(bins):\n",
    "        axs[i].errorbar(strategies, [binned_hits_at_k[bin][column][s]['1']['score'] for s in strategies], yerr=[[binned_hits_at_k[bin][column][s]['1']['score'] - binned_hits_at_k[bin][column][s]['1']['cl'][0] for s in strategies], [binned_hits_at_k[bin][column][s]['1']['cl'][1] - binned_hits_at_k[bin][column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "        \n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the full model with the model with random section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['model_ranking_corrupt_section_rank', 'model_ranking_corrupt_section_random_rank']\n",
    "labels = ['Correct Input', 'Random Section']\n",
    "markers = ['^', 'o']\n",
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Impact of Source Section Knowledge')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(\"MRR\" if i == 0 else \"Hits@1\")\n",
    "\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    # plot MRR@max in the first plot\n",
    "    # plot Hits@1 in the second plot\n",
    "    axs[0].errorbar(strategies, [stratified_mrr_at_k[column][s]['max']['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s]['max']['score'] - stratified_mrr_at_k[column][s]['max']['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s]['max']['cl'][1] - stratified_mrr_at_k[column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=10, markersize=8, linestyle='None', elinewidth=3, markeredgewidth=3)\n",
    "    axs[1].errorbar(strategies, [stratified_hits_at_k[column][s]['1']['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s]['1']['score'] - stratified_hits_at_k[column][s]['1']['cl'][0] for s in strategies], [stratified_hits_at_k[column][s]['1']['cl'][1] - stratified_hits_at_k[column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=10, markersize=8, linestyle='None', elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Vanilla BM25 with Mention-Aware BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "columns = ['bm25_rank', 'bm25_mentions_rank']\n",
    "labels = ['BM25', 'BM25 w/ Mentions']\n",
    "markers = ['v', 's']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('BM25 vs BM25 w/ Mentions')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(\"MRR\" if i == 0 else \"Hits@1\")\n",
    "\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[0].errorbar(strategies, [stratified_mrr_at_k[column][s]['max']['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s]['max']['score'] - stratified_mrr_at_k[column][s]['max']['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s]['max']['cl'][1] - stratified_mrr_at_k[column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "    axs[1].errorbar(strategies, [stratified_hits_at_k[column][s]['1']['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s]['1']['score'] - stratified_hits_at_k[column][s]['1']['cl'][0] for s in strategies], [stratified_hits_at_k[column][s]['1']['cl'][1] - stratified_hits_at_k[column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all the model variations with Ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "columns = ['model_ranking_rank', 'model_ranking_corrupt_rank', 'model_ranking_corrupt_section_rank', 'model_ranking_corrupt_section_mentions_rank', 'model_ranking_corrupt_section_mentions_negmask_rank']\n",
    "labels = ['(1): Fine-Tuning', '(2): (1) + Augmentation', '(3): (2) + Section Knowledge', '(4): (3) + Mentions Knowledge', '(5): (4) + Negative Masking']\n",
    "markers = ['^', 'o', 'v', 's', 'D']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Model Variations with Ranking Loss')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(\"MRR\" if i == 0 else \"Hits@1\")\n",
    "\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[0].errorbar(strategies, [stratified_mrr_at_k[column][s]['max']['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s]['max']['score'] - stratified_mrr_at_k[column][s]['max']['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s]['max']['cl'][1] - stratified_mrr_at_k[column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "    axs[1].errorbar(strategies, [stratified_hits_at_k[column][s]['1']['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s]['1']['score'] - stratified_hits_at_k[column][s]['1']['cl'][0] for s in strategies], [stratified_hits_at_k[column][s]['1']['cl'][1] - stratified_hits_at_k[column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Ranking Loss with Independent Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "columns = ['model_indep_rank', 'model_ranking_rank']\n",
    "labels = ['Independent Loss', 'Ranking Loss',]\n",
    "markers = ['^', 'o']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Ranking Loss vs Independent Loss (LM Fine-Tuning)')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(\"MRR\" if i == 0 else \"Hits@1\")\n",
    "\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[0].errorbar(strategies, [stratified_mrr_at_k[column][s]['max']['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s]['max']['score'] - stratified_mrr_at_k[column][s]['max']['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s]['max']['cl'][1] - stratified_mrr_at_k[column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "    axs[1].errorbar(strategies, [stratified_hits_at_k[column][s]['1']['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s]['1']['score'] - stratified_hits_at_k[column][s]['1']['cl'][0] for s in strategies], [stratified_hits_at_k[column][s]['1']['cl'][1] - stratified_hits_at_k[column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuned with Augmentations, Section Knowledge, Mention Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "columns = ['model_indep_corrupt_section_mentions_rank', 'model_ranking_corrupt_section_mentions_rank']\n",
    "labels = ['Independent Loss', 'Ranking Loss',]\n",
    "markers = ['^', 'o']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Ranking Loss vs Independent Loss (Fine-Tuning + Augmentations + Section Knowledge + Mentions Knowledge)')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(\"MRR\" if i == 1 else \"Hits@1\")\n",
    "\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[0].errorbar(strategies, [stratified_hits_at_k[column][s]['1']['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s]['1']['score'] - stratified_hits_at_k[column][s]['1']['cl'][0] for s in strategies], [stratified_hits_at_k[column][s]['1']['cl'][1] - stratified_hits_at_k[column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "    axs[1].errorbar(strategies, [stratified_mrr_at_k[column][s]['max']['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s]['max']['score'] - stratified_mrr_at_k[column][s]['max']['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s]['max']['cl'][1] - stratified_mrr_at_k[column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['present', 'missing_mention', 'missing_sentence', 'missing_span']\n",
    "columns = ['random_rank', 'bm25_mentions_rank', 'fuzzy_match_rank', 'model_ranking_rank', 'model_ranking_corrupt_section_mentions_rank']\n",
    "# columns = ['random_rank', 'bm25_mentions_rank', 'fuzzy_match_rank', 'model_ranking_corrupt_section_mentions_rank']\n",
    "labels = ['Random', 'BM25', 'String Match', 'LM Fine-Tuning', 'Ours']\n",
    "markers = ['o', 'v', 's', '^', 'D']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Ranking Performance Comparison')\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title(\"MRR\" if i == 1 else \"Hits@1\")\n",
    "\n",
    "for column, label, marker in zip(columns, labels, markers):\n",
    "    axs[0].errorbar(strategies, [stratified_hits_at_k[column][s]['1']['score'] for s in strategies], yerr=[[stratified_hits_at_k[column][s]['1']['score'] - stratified_hits_at_k[column][s]['1']['cl'][0] for s in strategies], [stratified_hits_at_k[column][s]['1']['cl'][1] - stratified_hits_at_k[column][s]['1']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "    axs[1].errorbar(strategies, [stratified_mrr_at_k[column][s]['max']['score'] for s in strategies], yerr=[[stratified_mrr_at_k[column][s]['max']['score'] - stratified_mrr_at_k[column][s]['max']['cl'][0] for s in strategies], [stratified_mrr_at_k[column][s]['max']['cl'][1] - stratified_mrr_at_k[column][s]['max']['score'] for s in strategies]], label=label, marker=marker, capsize=5, linestyle='None', markersize=8, elinewidth=3, markeredgewidth=3)\n",
    "\n",
    "handles, labels = axs[-1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower right')\n",
    "\n",
    "# rotate x ticks\n",
    "for ax in axs:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['missing_category'] == 'present']['bm25_mentions_rank'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[(test_df['missing_category'] == 'present') & (test_df['bm25_mentions_rank'] != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_dump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
