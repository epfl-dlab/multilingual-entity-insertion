{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {'af': {'full': 'Afrikaans'},\n",
    "             'cs': {'full': 'Czech'},\n",
    "             'cy': {'full': 'Welsh'},\n",
    "             'en': {'full': 'English'},\n",
    "             'fr': {'full': 'French'},\n",
    "             'ga': {'full': 'Irish'},\n",
    "             'gu': {'full': 'Gujarati'},\n",
    "             'hi': {'full': 'Hindi'},\n",
    "             'is': {'full': 'Icelandic'},\n",
    "             'it': {'full': 'Italian'},\n",
    "             'ja': {'full': 'Japanese'},\n",
    "             'kk': {'full': 'Kazakh'},\n",
    "             'kn': {'full': 'Kannada'},\n",
    "             'ms': {'full': 'Malay'},\n",
    "             'ps': {'full': 'Pashto'},\n",
    "             'pt': {'full': 'Portuguese'},\n",
    "             'simple': {'full': 'Simple English'},\n",
    "             'sk': {'full': 'Slovak'},\n",
    "             'sw': {'full': 'Swahili'},\n",
    "             'ur': {'full': 'Urdu'},\n",
    "             'uz': {'full': 'Uzbek'},\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "    languages[language]['total_dirty'] = 0\n",
    "    languages[language]['missing_context'] = 0\n",
    "    languages[language]['missing_negative_contexts'] = 0\n",
    "    languages[language]['bad_target'] = 0\n",
    "    languages[language]['missing_section'] = 0\n",
    "    languages[language]['total_clean'] = 0\n",
    "    languages[language]['total_clean_eval'] = 0 # also excluding the missing section because our model doesn't apply to that\n",
    "    languages[language]['present_text'] = 0\n",
    "    languages[language]['missing_mention'] = 0\n",
    "    languages[language]['missing_sentence'] = 0\n",
    "    languages[language]['missing_span'] = 0\n",
    "    languages[language]['missing_section'] = 0\n",
    "    languages[language]['candidates'] = 0\n",
    "\n",
    "language_keys = list(languages.keys())\n",
    "for language in (pbar := tqdm(language_keys)):\n",
    "    if not os.path.exists(f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231001/eval/test_data.parquet'):\n",
    "        del languages[language]\n",
    "        continue\n",
    "    good_pages = []\n",
    "    files = glob(f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231001/processed_data/good_pages/*.parquet') + \\\n",
    "            glob(f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231101/processed_data/good_pages/*.parquet')\n",
    "    for i, file in enumerate(files):\n",
    "        pbar.set_description(f'{language} - {i} / {len(files)}')\n",
    "        df = pd.read_parquet(file, columns=['title'])\n",
    "        good_pages.extend(list(df['title'].values))\n",
    "    good_pages = set(good_pages)\n",
    "    languages[language]['good_pages'] = good_pages\n",
    "    # read test data in chunks\n",
    "    # only read columns: context, negative_contexts, target_title, missing_category\n",
    "    parquet_file = pq.ParquetFile(f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231001/eval/test_data.parquet')\n",
    "    num_rows = parquet_file.metadata.num_rows\n",
    "    for batch in parquet_file.iter_batches(batch_size=10_000):\n",
    "        pbar.set_description(f'{language} - {languages[language][\"total_dirty\"]} / {num_rows}')\n",
    "        df = batch.to_pandas()\n",
    "        \n",
    "        languages[language]['total_dirty'] += len(df)\n",
    "        \n",
    "        no_context = df['context'] == ''\n",
    "        languages[language]['missing_context'] += no_context.sum()\n",
    "        \n",
    "        no_neg_contexts = df['negative_contexts'] == '[]'\n",
    "        languages[language]['missing_negative_contexts'] += no_neg_contexts.sum()\n",
    "        \n",
    "        bad_target = ~df['target_title'].isin(languages[language]['good_pages'])\n",
    "        languages[language]['bad_target'] += bad_target.sum()\n",
    "\n",
    "        df_clean = df[~no_context & ~no_neg_contexts & ~bad_target]\n",
    "        languages[language]['total_clean'] += len(df_clean)\n",
    "        languages[language]['total_clean_eval'] += len(df_clean[df_clean['missing_category'] != 'missing_section'])\n",
    "        \n",
    "        # find the value counts for column \"missing category\"\n",
    "        # include NA values\n",
    "        categories = df_clean['missing_category'].value_counts(dropna=False)\n",
    "        languages[language]['present_text'] += categories[None] if None in categories.index else 0\n",
    "        languages[language]['missing_mention'] += categories['missing_mention'] if 'missing_mention' in categories.index else 0\n",
    "        languages[language]['missing_sentence'] += categories['missing_sentence'] if 'missing_sentence' in categories.index else 0\n",
    "        languages[language]['missing_span'] += categories['missing_span'] if 'missing_span' in categories.index else 0\n",
    "        languages[language]['missing_section'] += categories['missing_section'] if 'missing_section' in categories.index else 0\n",
    "        languages[language]['candidates'] += df_clean['negative_contexts'].reset_index(drop=True).apply(lambda x: x.count(\"'context'\") + 1).mean()\n",
    "\n",
    "    languages[language]['present_text'] /= languages[language]['total_clean'] / 100\n",
    "    languages[language]['missing_mention'] /= languages[language]['total_clean'] / 100\n",
    "    languages[language]['missing_sentence'] /= languages[language]['total_clean'] / 100\n",
    "    languages[language]['missing_span'] /= languages[language]['total_clean'] / 100\n",
    "    languages[language]['missing_section'] /= languages[language]['total_clean'] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    if languages[lang]['total_clean'] < 1_000:\n",
    "        languages[lang]['total_text'] = languages[lang]['total_clean']\n",
    "    elif languages[lang]['total_clean'] < 1_000_000:\n",
    "        languages[lang]['total_text'] = f'{round(languages[lang][\"total_clean\"] / 1_000):d}K'\n",
    "    else:\n",
    "        languages[lang]['total_text'] = f'{round(languages[lang][\"total_clean\"] / 1_000_000):d}M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib stacked bar plot\n",
    "# create a 1x4 grid of subplots\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 15), width_ratios=[7, 7, 7, 3])\n",
    "\n",
    "# set global title\n",
    "fig.suptitle('Link Insertion Strategy Distribution (Oct 2023 $\\\\rightarrow$ Nov 2023)', fontsize=60, y=1.1, weight='bold')\n",
    "# set y label for all subplots\n",
    "for ax in axs:\n",
    "    # ax.set_xlabel('Language', fontsize=20)\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=30)\n",
    "    # set to bold\n",
    "    ax.tick_params(axis='x', which='major', labelsize=30)\n",
    "axs[0].set_ylabel('Percentage of Links', fontsize=50, weight='bold')\n",
    "languages = {k: v for k, v in sorted(languages.items(), key=lambda item: item[1]['total_clean'], reverse=False)}\n",
    "labels = [f\"{language}\\n({languages[language]['total_text']})\" if language != 'en' else \\\n",
    "        f\"{language}\\n({languages[language]['total_text']}*)\" for language in languages]\n",
    "labels.append('Micro\\nAverage')\n",
    "labels.append('Macro\\nAverage')\n",
    "labels = [labels[0:7], labels[7:14], labels[14:21], [labels[21],labels[22]]]\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.set_xticks(np.arange(len(labels[i])))\n",
    "    ax.set_xticklabels(labels[i], rotation=0, fontsize=29, weight='bold')\n",
    "    ax.set_yticks([])\n",
    "axs[0].set_yticks(np.arange(0, 101, 25))\n",
    "axs[0].set_yticklabels(np.arange(0, 101, 25), fontsize=30, weight='bold')\n",
    "\n",
    "present_text = []\n",
    "missing_mention = []\n",
    "missing_sentence = []\n",
    "missing_span = []\n",
    "missing_section = []\n",
    "\n",
    "total = {'present_text': 0,\n",
    "         'missing_mention': 0,\n",
    "         'missing_sentence': 0,\n",
    "         'missing_span': 0,\n",
    "         'missing_section': 0,\n",
    "         'total': 0}\n",
    "for language in languages:\n",
    "    present_text.append(languages[language]['present_text'])\n",
    "    missing_mention.append(languages[language]['missing_mention'])\n",
    "    missing_sentence.append(languages[language]['missing_sentence'])\n",
    "    missing_span.append(languages[language]['missing_span'])\n",
    "    missing_section.append(languages[language]['missing_section'])\n",
    "    total['present_text'] += languages[language]['present_text'] * languages[language]['total_clean'] / 100\n",
    "    total['missing_mention'] += languages[language]['missing_mention'] * languages[language]['total_clean'] / 100\n",
    "    total['missing_sentence'] += languages[language]['missing_sentence'] * languages[language]['total_clean'] / 100\n",
    "    total['missing_span'] += languages[language]['missing_span'] * languages[language]['total_clean'] / 100\n",
    "    total['missing_section'] += languages[language]['missing_section'] * languages[language]['total_clean'] / 100\n",
    "    total['total'] += languages[language]['total_clean']\n",
    "\n",
    "# append micro average results\n",
    "present_text.append(total['present_text'] / total['total'] * 100)\n",
    "missing_mention.append(total['missing_mention'] / total['total'] * 100)\n",
    "missing_sentence.append(total['missing_sentence'] / total['total'] * 100)\n",
    "missing_span.append(total['missing_span'] / total['total'] * 100)\n",
    "missing_section.append(total['missing_section'] / total['total'] * 100)\n",
    "# append macro average results\n",
    "present_text.append(sum(present_text[:-1]) / len(present_text[:-1]))\n",
    "missing_mention.append(sum(missing_mention[:-1]) / len(missing_mention[:-1]))\n",
    "missing_sentence.append(sum(missing_sentence[:-1]) / len(missing_sentence[:-1]))\n",
    "missing_span.append(sum(missing_span[:-1]) / len(missing_span[:-1]))\n",
    "missing_section.append(sum(missing_section[:-1]) / len(missing_section[:-1]))\n",
    "\n",
    "# split into 4 subplots\n",
    "present_text = [present_text[0:7], present_text[7:14], present_text[14:21], [present_text[21], present_text[22]]]\n",
    "missing_mention = [missing_mention[0:7], missing_mention[7:14], missing_mention[14:21], [missing_mention[21], missing_mention[22]]]\n",
    "missing_sentence = [missing_sentence[0:7], missing_sentence[7:14], missing_sentence[14:21], [missing_sentence[21], missing_sentence[22]]]\n",
    "missing_span = [missing_span[0:7], missing_span[7:14], missing_span[14:21], [missing_span[21], missing_span[22]]]\n",
    "missing_section = [missing_section[0:7], missing_section[7:14], missing_section[14:21], [missing_section[21], missing_section[22]]]\n",
    "\n",
    "bottom = [np.zeros(len(p)) for p in present_text]\n",
    "present_text = [np.array(p) for p in present_text]\n",
    "missing_mention = [np.array(p) for p in missing_mention]\n",
    "missing_sentence = [np.array(p) for p in missing_sentence]\n",
    "missing_span = [np.array(p) for p in missing_span]\n",
    "missing_section = [np.array(p) for p in missing_section]\n",
    "\n",
    "# use a colorblind friendly palette\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.bar(np.arange(len(present_text[i])), present_text[i], label='Text Present', bottom=bottom[i], width=0.85, color=sns.color_palette('colorblind')[0])\n",
    "    bottom[i] += present_text[i]\n",
    "    ax.bar(np.arange(len(present_text[i])), missing_mention[i], label='Mention Missing', bottom=bottom[i], width=0.85, color=sns.color_palette('colorblind')[1])\n",
    "    bottom[i] += missing_mention[i]\n",
    "    ax.bar(np.arange(len(present_text[i])), missing_sentence[i], label='Sentence Missing', bottom=bottom[i], width=0.85, color=sns.color_palette('colorblind')[2])\n",
    "    bottom[i] += missing_sentence[i]\n",
    "    ax.bar(np.arange(len(present_text[i])), missing_span[i], label='Span Missing', bottom=bottom[i], width=0.85, color=sns.color_palette('colorblind')[3])\n",
    "    bottom[i] += missing_span[i]\n",
    "    ax.bar(np.arange(len(present_text[i])), missing_section[i], label='Section Missing', bottom=bottom[i], width=0.85, color=sns.color_palette('colorblind')[4])\n",
    "    bottom[i] += missing_section[i]\n",
    "\n",
    "# add legend to figure\n",
    "# only show the legend for the first subplot\n",
    "# make the legend horizontal and centered\n",
    "# axs[0].legend(bbox_to_anchor=(2, 1.1), ncol=5, fontsize=20)\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "fig.legend(handles=handles, labels=labels, loc='upper center', ncol=5, fontsize=45, bbox_to_anchor=(0.5, 1.05))\n",
    "# reverse the order of the legend\n",
    "# handles, labels = axs[0].get_legend_handles_labels()\n",
    "# axs[0].legend(handles[::-1], labels[::-1],bbox_to_anchor=(1, 1.07), ncol=5, fontsize=18)\n",
    "\n",
    "\n",
    "# add labels inside each bar\n",
    "for row in range(len(axs)):\n",
    "    for i, v in enumerate(present_text[row]):\n",
    "        if v > 0:\n",
    "            axs[row].text(i, v/2, f'{v:.0f}%', color='white', ha='center', va='center', fontweight='bold', fontsize=25)\n",
    "    for i, v in enumerate(missing_mention[row]):\n",
    "        if v > 0:\n",
    "            axs[row].text(i, present_text[row][i]+v/2, f'{v:.0f}%', color='white', ha='center', va='center', fontweight='bold', fontsize=25)\n",
    "    for i, v in enumerate(missing_sentence[row]):\n",
    "        if v > 0:\n",
    "            axs[row].text(i, present_text[row][i]+missing_mention[row][i]+v/2, f'{v:.0f}%', color='white', ha='center', va='center', fontweight='bold', fontsize=25)\n",
    "    for i, v in enumerate(missing_span[row]):\n",
    "        if v > 0:\n",
    "            axs[row].text(i, present_text[row][i]+missing_mention[row][i]+missing_sentence[row][i]+v/2, f'{v:.0f}%', color='white', ha='center', va='center', fontweight='bold', fontsize=25)\n",
    "    for i, v in enumerate(missing_section[row]):\n",
    "        if v > 0:\n",
    "            axs[row].text(i, present_text[row][i]+missing_mention[row][i]+missing_sentence[row][i]+missing_span[row][i]+v/2, f'{v:.0f}%', color='white', ha='center', va='center', fontweight='bold', fontsize=25)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a bar chart of the number of negative contexts per language\n",
    "fig, ax = plt.subplots(figsize=(35, 10))\n",
    "ax.set_title('Number of Candidates per Language')\n",
    "ax.set_ylabel('Number of Candidates')\n",
    "ax.set_xlabel('Language')\n",
    "# sort the languages by number of candidates\n",
    "languages = {k: v for k, v in sorted(languages.items(), key=lambda item: item[1]['candidates'])}\n",
    "ax.set_xticks(np.arange(len(languages)))\n",
    "ax.set_xticklabels([languages[language]['full'] for language in languages], rotation=0)\n",
    "ax.bar(np.arange(len(languages)), [languages[language]['candidates'] for language in languages])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110,\n",
       " ['af',\n",
       "  'am',\n",
       "  'an',\n",
       "  'ar',\n",
       "  'ary',\n",
       "  'as',\n",
       "  'ast',\n",
       "  'az',\n",
       "  'azb',\n",
       "  'ba',\n",
       "  'bar',\n",
       "  'be',\n",
       "  'bg',\n",
       "  'br',\n",
       "  'bs',\n",
       "  'ca',\n",
       "  'ce',\n",
       "  'cs',\n",
       "  'cv',\n",
       "  'cy',\n",
       "  'da',\n",
       "  'de',\n",
       "  'el',\n",
       "  'en',\n",
       "  'eo',\n",
       "  'es',\n",
       "  'et',\n",
       "  'eu',\n",
       "  'fa',\n",
       "  'fi',\n",
       "  'fr',\n",
       "  'fy',\n",
       "  'ga',\n",
       "  'gd',\n",
       "  'gl',\n",
       "  'gu',\n",
       "  'ha',\n",
       "  'he',\n",
       "  'hi',\n",
       "  'hr',\n",
       "  'hu',\n",
       "  'hy',\n",
       "  'id',\n",
       "  'io',\n",
       "  'is',\n",
       "  'it',\n",
       "  'ja',\n",
       "  'jv',\n",
       "  'ka',\n",
       "  'kk',\n",
       "  'km',\n",
       "  'kn',\n",
       "  'ko',\n",
       "  'ku',\n",
       "  'la',\n",
       "  'lb',\n",
       "  'lmo',\n",
       "  'lo',\n",
       "  'lt',\n",
       "  'lv',\n",
       "  'mg',\n",
       "  'min',\n",
       "  'mk',\n",
       "  'mn',\n",
       "  'mr',\n",
       "  'ms',\n",
       "  'nds-nl',\n",
       "  'ne',\n",
       "  'nl',\n",
       "  'no',\n",
       "  'oc',\n",
       "  'om',\n",
       "  'pa',\n",
       "  'pl',\n",
       "  'pms',\n",
       "  'ps',\n",
       "  'pt',\n",
       "  'ro',\n",
       "  'ru',\n",
       "  'sa',\n",
       "  'scn',\n",
       "  'sco',\n",
       "  'sd',\n",
       "  'sh',\n",
       "  'si',\n",
       "  'simple',\n",
       "  'sk',\n",
       "  'sl',\n",
       "  'so',\n",
       "  'sq',\n",
       "  'sr',\n",
       "  'su',\n",
       "  'sv',\n",
       "  'sw',\n",
       "  'ta',\n",
       "  'te',\n",
       "  'tg',\n",
       "  'tl',\n",
       "  'tr',\n",
       "  'tt',\n",
       "  'ug',\n",
       "  'uk',\n",
       "  'ur',\n",
       "  'uz',\n",
       "  'vi',\n",
       "  'vo',\n",
       "  'war',\n",
       "  'xh',\n",
       "  'yi',\n",
       "  'zh'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('/dlabdata1/tsoares/wikidumps/*wiki-NS0-20231001/eval/')\n",
    "# find the list of all languages\n",
    "languages = [file.split('/')[-3][:-17] for file in files]\n",
    "languages.sort()\n",
    "len(languages), languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "zh - 25000 / 45658: 100%|██████████| 110/110 [18:51<00:00, 10.28s/it]   \n"
     ]
    }
   ],
   "source": [
    "stats = {lang: {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0,\n",
    "                'missing_span': 0, 'missing_section': 0} for lang in languages}\n",
    "\n",
    "for language in (pbar := tqdm(languages)):\n",
    "    if not os.path.exists(f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231001/eval/test_data.parquet'):\n",
    "        continue\n",
    "    good_pages = []\n",
    "    files = glob(f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231001/processed_data/good_pages/*.parquet') + \\\n",
    "        glob(\n",
    "            f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231101/processed_data/good_pages/*.parquet')\n",
    "    for i, file in enumerate(files):\n",
    "        pbar.set_description(f'{language} - {i} / {len(files)}')\n",
    "        try:\n",
    "            df = pd.read_parquet(file, columns=['title'])\n",
    "        except:\n",
    "            pass\n",
    "        good_pages.extend(list(df['title'].values))\n",
    "    good_pages = set(good_pages)\n",
    "    # read test data in chunks\n",
    "    # only read columns: context, negative_contexts, target_title, missing_category\n",
    "    parquet_file = pq.ParquetFile(\n",
    "        f'/dlabdata1/tsoares/wikidumps/{language}wiki-NS0-20231001/eval/test_data.parquet')\n",
    "    num_rows = parquet_file.metadata.num_rows\n",
    "    total = 0\n",
    "    for batch in parquet_file.iter_batches(batch_size=25_000):\n",
    "        pbar.set_description(\n",
    "            f'{language} - {total} / {num_rows}')\n",
    "        df = batch.to_pandas()\n",
    "        total += len(df)\n",
    "        no_context = df['context'] == ''\n",
    "        no_neg_contexts = df['negative_contexts'] == '[]'\n",
    "        bad_target = ~df['target_title'].isin(good_pages)\n",
    "\n",
    "        df_clean = df[~no_context & ~no_neg_contexts & ~bad_target]\n",
    "\n",
    "        # find the value counts for column \"missing category\"\n",
    "        # include NA values\n",
    "        categories = df_clean['missing_category'].value_counts(dropna=False)\n",
    "        stats[language]['text_present'] += categories[None] if None in categories.index else 0\n",
    "        stats[language]['missing_mention'] += categories['missing_mention'] if 'missing_mention' in categories.index else 0\n",
    "        stats[language]['missing_sentence'] += categories['missing_sentence'] if 'missing_sentence' in categories.index else 0\n",
    "        stats[language]['missing_span'] += categories['missing_span'] if 'missing_span' in categories.index else 0\n",
    "        stats[language]['missing_section'] += categories['missing_section'] if 'missing_section' in categories.index else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(stats).T\n",
    "stats_df.to_csv('stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats \u001b[38;5;241m=\u001b[39m [(k, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(stats)\n\u001b[1;32m      3\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(stats, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "stats = [(k, v) for k, v in stats.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('af', {'text_present': 695, 'missing_mention': 104, 'missing_sentence': 161, 'missing_span': 262, 'missing_section': 227}), ('am', {'text_present': 9, 'missing_mention': 3, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('an', {'text_present': 8, 'missing_mention': 6, 'missing_sentence': 13, 'missing_span': 18, 'missing_section': 25}), ('ar', {'text_present': 7229, 'missing_mention': 2661, 'missing_sentence': 1670, 'missing_span': 3996, 'missing_section': 2304}), ('ary', {'text_present': 43, 'missing_mention': 29, 'missing_sentence': 14, 'missing_span': 32, 'missing_section': 11}), ('as', {'text_present': 214, 'missing_mention': 77, 'missing_sentence': 21, 'missing_span': 54, 'missing_section': 93}), ('ast', {'text_present': 14, 'missing_mention': 22, 'missing_sentence': 1, 'missing_span': 11, 'missing_section': 1}), ('az', {'text_present': 1982, 'missing_mention': 703, 'missing_sentence': 666, 'missing_span': 2586, 'missing_section': 376}), ('azb', {'text_present': 15, 'missing_mention': 3, 'missing_sentence': 4, 'missing_span': 0, 'missing_section': 0}), ('ba', {'text_present': 212, 'missing_mention': 60, 'missing_sentence': 21, 'missing_span': 206, 'missing_section': 150}), ('bar', {'text_present': 9, 'missing_mention': 2, 'missing_sentence': 2, 'missing_span': 7, 'missing_section': 1}), ('be', {'text_present': 839, 'missing_mention': 386, 'missing_sentence': 554, 'missing_span': 876, 'missing_section': 437}), ('bg', {'text_present': 1546, 'missing_mention': 752, 'missing_sentence': 400, 'missing_span': 1578, 'missing_section': 531}), ('br', {'text_present': 222, 'missing_mention': 197, 'missing_sentence': 116, 'missing_span': 198, 'missing_section': 119}), ('bs', {'text_present': 352, 'missing_mention': 54, 'missing_sentence': 144, 'missing_span': 257, 'missing_section': 162}), ('ca', {'text_present': 4506, 'missing_mention': 2425, 'missing_sentence': 1518, 'missing_span': 6783, 'missing_section': 3178}), ('ce', {'text_present': 0, 'missing_mention': 2, 'missing_sentence': 0, 'missing_span': 45, 'missing_section': 1}), ('cs', {'text_present': 3687, 'missing_mention': 1990, 'missing_sentence': 1556, 'missing_span': 3201, 'missing_section': 1870}), ('cv', {'text_present': 72, 'missing_mention': 67, 'missing_sentence': 51, 'missing_span': 74, 'missing_section': 40}), ('cy', {'text_present': 224, 'missing_mention': 56, 'missing_sentence': 41, 'missing_span': 44, 'missing_section': 21}), ('da', {'text_present': 679, 'missing_mention': 369, 'missing_sentence': 298, 'missing_span': 619, 'missing_section': 398}), ('de', {'text_present': 9158, 'missing_mention': 4937, 'missing_sentence': 3330, 'missing_span': 8649, 'missing_section': 3547}), ('el', {'text_present': 3032, 'missing_mention': 824, 'missing_sentence': 445, 'missing_span': 1937, 'missing_section': 505}), ('en', {'text_present': 41492, 'missing_mention': 33679, 'missing_sentence': 32583, 'missing_span': 48325, 'missing_section': 17808}), ('eo', {'text_present': 1218, 'missing_mention': 2237, 'missing_sentence': 576, 'missing_span': 1045, 'missing_section': 762}), ('es', {'text_present': 20113, 'missing_mention': 19183, 'missing_sentence': 5899, 'missing_span': 13763, 'missing_section': 7419}), ('et', {'text_present': 1172, 'missing_mention': 857, 'missing_sentence': 813, 'missing_span': 1201, 'missing_section': 662}), ('eu', {'text_present': 1047, 'missing_mention': 434, 'missing_sentence': 1515, 'missing_span': 1543, 'missing_section': 603}), ('fa', {'text_present': 7615, 'missing_mention': 3447, 'missing_sentence': 1347, 'missing_span': 3571, 'missing_section': 2127}), ('fi', {'text_present': 5187, 'missing_mention': 2162, 'missing_sentence': 1924, 'missing_span': 2907, 'missing_section': 1548}), ('fr', {'text_present': 19626, 'missing_mention': 11290, 'missing_sentence': 10479, 'missing_span': 18293, 'missing_section': 4851}), ('fy', {'text_present': 187, 'missing_mention': 83, 'missing_sentence': 50, 'missing_span': 183, 'missing_section': 76}), ('ga', {'text_present': 61, 'missing_mention': 34, 'missing_sentence': 18, 'missing_span': 79, 'missing_section': 12}), ('gd', {'text_present': 4, 'missing_mention': 2, 'missing_sentence': 5, 'missing_span': 3, 'missing_section': 0}), ('gl', {'text_present': 1030, 'missing_mention': 461, 'missing_sentence': 307, 'missing_span': 1847, 'missing_section': 613}), ('gu', {'text_present': 18, 'missing_mention': 5, 'missing_sentence': 0, 'missing_span': 4, 'missing_section': 2}), ('ha', {'text_present': 201, 'missing_mention': 11, 'missing_sentence': 4, 'missing_span': 17, 'missing_section': 29}), ('he', {'text_present': 13190, 'missing_mention': 5837, 'missing_sentence': 5087, 'missing_span': 9014, 'missing_section': 3653}), ('hi', {'text_present': 1002, 'missing_mention': 721, 'missing_sentence': 84, 'missing_span': 156, 'missing_section': 379}), ('hr', {'text_present': 1107, 'missing_mention': 531, 'missing_sentence': 220, 'missing_span': 719, 'missing_section': 876}), ('hu', {'text_present': 2422, 'missing_mention': 1193, 'missing_sentence': 1013, 'missing_span': 2467, 'missing_section': 708}), ('hy', {'text_present': 1354, 'missing_mention': 1007, 'missing_sentence': 387, 'missing_span': 625, 'missing_section': 426}), ('id', {'text_present': 3267, 'missing_mention': 2216, 'missing_sentence': 1749, 'missing_span': 4609, 'missing_section': 1886}), ('io', {'text_present': 14, 'missing_mention': 46, 'missing_sentence': 65, 'missing_span': 80, 'missing_section': 25}), ('is', {'text_present': 158, 'missing_mention': 75, 'missing_sentence': 144, 'missing_span': 559, 'missing_section': 109}), ('it', {'text_present': 12783, 'missing_mention': 7754, 'missing_sentence': 7516, 'missing_span': 11158, 'missing_section': 6430}), ('ja', {'text_present': 22607, 'missing_mention': 21860, 'missing_sentence': 17935, 'missing_span': 12282, 'missing_section': 4374}), ('jv', {'text_present': 66, 'missing_mention': 21, 'missing_sentence': 3, 'missing_span': 46, 'missing_section': 25}), ('ka', {'text_present': 1106, 'missing_mention': 233, 'missing_sentence': 87, 'missing_span': 222, 'missing_section': 138}), ('kk', {'text_present': 942, 'missing_mention': 309, 'missing_sentence': 85, 'missing_span': 140, 'missing_section': 119}), ('km', {'text_present': 62, 'missing_mention': 22, 'missing_sentence': 3, 'missing_span': 4, 'missing_section': 4}), ('kn', {'text_present': 315, 'missing_mention': 27, 'missing_sentence': 15, 'missing_span': 83, 'missing_section': 74}), ('ko', {'text_present': 5964, 'missing_mention': 3509, 'missing_sentence': 4009, 'missing_span': 5425, 'missing_section': 2491}), ('ku', {'text_present': 263, 'missing_mention': 130, 'missing_sentence': 31, 'missing_span': 163, 'missing_section': 27}), ('la', {'text_present': 357, 'missing_mention': 266, 'missing_sentence': 220, 'missing_span': 315, 'missing_section': 71}), ('lb', {'text_present': 184, 'missing_mention': 148, 'missing_sentence': 168, 'missing_span': 117, 'missing_section': 137}), ('lmo', {'text_present': 3, 'missing_mention': 2, 'missing_sentence': 1, 'missing_span': 1, 'missing_section': 19}), ('lo', {'text_present': 4, 'missing_mention': 3, 'missing_sentence': 3, 'missing_span': 1, 'missing_section': 0}), ('lt', {'text_present': 497, 'missing_mention': 372, 'missing_sentence': 450, 'missing_span': 606, 'missing_section': 488}), ('lv', {'text_present': 541, 'missing_mention': 328, 'missing_sentence': 229, 'missing_span': 500, 'missing_section': 358}), ('mg', {'text_present': 406, 'missing_mention': 164, 'missing_sentence': 55, 'missing_span': 157, 'missing_section': 421}), ('min', {'text_present': 11, 'missing_mention': 2, 'missing_sentence': 0, 'missing_span': 1, 'missing_section': 7}), ('mk', {'text_present': 331, 'missing_mention': 157, 'missing_sentence': 120, 'missing_span': 202, 'missing_section': 113}), ('mn', {'text_present': 250, 'missing_mention': 68, 'missing_sentence': 26, 'missing_span': 81, 'missing_section': 42}), ('mr', {'text_present': 46, 'missing_mention': 10, 'missing_sentence': 6, 'missing_span': 4, 'missing_section': 1}), ('ms', {'text_present': 897, 'missing_mention': 710, 'missing_sentence': 298, 'missing_span': 414, 'missing_section': 427}), ('nds-nl', {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('ne', {'text_present': 100, 'missing_mention': 94, 'missing_sentence': 5, 'missing_span': 10, 'missing_section': 41}), ('nl', {'text_present': 6021, 'missing_mention': 5130, 'missing_sentence': 4085, 'missing_span': 5465, 'missing_section': 2996}), ('no', {'text_present': 1673, 'missing_mention': 1053, 'missing_sentence': 1048, 'missing_span': 2543, 'missing_section': 966}), ('oc', {'text_present': 552, 'missing_mention': 210, 'missing_sentence': 164, 'missing_span': 857, 'missing_section': 129}), ('om', {'text_present': 13, 'missing_mention': 0, 'missing_sentence': 1, 'missing_span': 3, 'missing_section': 1}), ('pa', {'text_present': 23, 'missing_mention': 40, 'missing_sentence': 39, 'missing_span': 13, 'missing_section': 24}), ('pl', {'text_present': 7488, 'missing_mention': 3956, 'missing_sentence': 4455, 'missing_span': 7525, 'missing_section': 3873}), ('pms', {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('ps', {'text_present': 5, 'missing_mention': 5, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('pt', {'text_present': 6677, 'missing_mention': 3728, 'missing_sentence': 2477, 'missing_span': 7109, 'missing_section': 4279}), ('ro', {'text_present': 1103, 'missing_mention': 546, 'missing_sentence': 779, 'missing_span': 1364, 'missing_section': 495}), ('ru', {'text_present': 12, 'missing_mention': 3, 'missing_sentence': 3, 'missing_span': 0, 'missing_section': 0}), ('sa', {'text_present': 4, 'missing_mention': 5, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 9}), ('scn', {'text_present': 0, 'missing_mention': 3, 'missing_sentence': 2, 'missing_span': 0, 'missing_section': 0}), ('sco', {'text_present': 18, 'missing_mention': 15, 'missing_sentence': 3, 'missing_span': 2, 'missing_section': 2}), ('sd', {'text_present': 8, 'missing_mention': 1, 'missing_sentence': 3, 'missing_span': 2, 'missing_section': 0}), ('sh', {'text_present': 217, 'missing_mention': 107, 'missing_sentence': 94, 'missing_span': 279, 'missing_section': 110}), ('si', {'text_present': 9, 'missing_mention': 3, 'missing_sentence': 3, 'missing_span': 16, 'missing_section': 5}), ('simple', {'text_present': 1344, 'missing_mention': 621, 'missing_sentence': 600, 'missing_span': 734, 'missing_section': 532}), ('sk', {'text_present': 1028, 'missing_mention': 497, 'missing_sentence': 384, 'missing_span': 782, 'missing_section': 432}), ('sl', {'text_present': 409, 'missing_mention': 273, 'missing_sentence': 123, 'missing_span': 494, 'missing_section': 245}), ('so', {'text_present': 29, 'missing_mention': 10, 'missing_sentence': 3, 'missing_span': 17, 'missing_section': 1}), ('sq', {'text_present': 128, 'missing_mention': 77, 'missing_sentence': 85, 'missing_span': 131, 'missing_section': 102}), ('sr', {'text_present': 2251, 'missing_mention': 735, 'missing_sentence': 514, 'missing_span': 1310, 'missing_section': 627}), ('su', {'text_present': 3, 'missing_mention': 0, 'missing_sentence': 1, 'missing_span': 2, 'missing_section': 0}), ('sv', {'text_present': 3453, 'missing_mention': 1978, 'missing_sentence': 1391, 'missing_span': 2610, 'missing_section': 1297}), ('sw', {'text_present': 173, 'missing_mention': 200, 'missing_sentence': 100, 'missing_span': 54, 'missing_section': 31}), ('ta', {'text_present': 362, 'missing_mention': 205, 'missing_sentence': 67, 'missing_span': 315, 'missing_section': 197}), ('te', {'text_present': 260, 'missing_mention': 158, 'missing_sentence': 95, 'missing_span': 314, 'missing_section': 333}), ('tg', {'text_present': 51, 'missing_mention': 38, 'missing_sentence': 6, 'missing_span': 21, 'missing_section': 7}), ('tl', {'text_present': 115, 'missing_mention': 114, 'missing_sentence': 18, 'missing_span': 144, 'missing_section': 121}), ('tr', {'text_present': 4474, 'missing_mention': 2427, 'missing_sentence': 1652, 'missing_span': 3983, 'missing_section': 2449}), ('tt', {'text_present': 17, 'missing_mention': 13, 'missing_sentence': 35, 'missing_span': 9, 'missing_section': 20}), ('ug', {'text_present': 0, 'missing_mention': 1, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('uk', {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('ur', {'text_present': 3941, 'missing_mention': 666, 'missing_sentence': 157, 'missing_span': 200, 'missing_section': 304}), ('uz', {'text_present': 2631, 'missing_mention': 318, 'missing_sentence': 172, 'missing_span': 527, 'missing_section': 690}), ('vi', {'text_present': 3797, 'missing_mention': 1562, 'missing_sentence': 999, 'missing_span': 4117, 'missing_section': 1510}), ('vo', {'text_present': 0, 'missing_mention': 7, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('war', {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('xh', {'text_present': 1, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0}), ('yi', {'text_present': 12, 'missing_mention': 2, 'missing_sentence': 1, 'missing_span': 6, 'missing_section': 0}), ('zh', {'text_present': 7870, 'missing_mention': 6582, 'missing_sentence': 4600, 'missing_span': 4641, 'missing_section': 1562})]\n"
     ]
    }
   ],
   "source": [
    "stats = sorted(stats, key = lambda x: sum(x[1].values()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en & 41492 (23.9 \\%) & 33679 (19.4 \\%) & 32583 (18.7 \\%) & 48325 (27.8 \\%) & 17808 (10.2 \\%) & 173887 & ja & 22607 (28.6 \\%) & 21860 (27.7 \\%) & 17935 (22.7 \\%) & 12282 (15.5 \\%) & 4374 (5.5 \\%) & 79058 \\\\\n",
      "es & 20113 (30.3 \\%) & 19183 (28.9 \\%) & 5899 (8.9 \\%) & 13763 (20.7 \\%) & 7419 (11.2 \\%) & 66377 & fr & 19626 (30.4 \\%) & 11290 (17.5 \\%) & 10479 (16.2 \\%) & 18293 (28.3 \\%) & 4851 (7.5 \\%) & 64539 \\\\\n",
      "it & 12783 (28.0 \\%) & 7754 (17.0 \\%) & 7516 (16.5 \\%) & 11158 (24.4 \\%) & 6430 (14.1 \\%) & 45641 & he & 13190 (35.9 \\%) & 5837 (15.9 \\%) & 5087 (13.8 \\%) & 9014 (24.5 \\%) & 3653 (9.9 \\%) & 36781 \\\\\n",
      "pl & 7488 (27.4 \\%) & 3956 (14.5 \\%) & 4455 (16.3 \\%) & 7525 (27.6 \\%) & 3873 (14.2 \\%) & 27297 & zh & 7870 (31.2 \\%) & 6582 (26.1 \\%) & 4600 (18.2 \\%) & 4641 (18.4 \\%) & 1562 (6.2 \\%) & 25255 \\\\\n",
      "pt & 6677 (27.5 \\%) & 3728 (15.4 \\%) & 2477 (10.2 \\%) & 7109 (29.3 \\%) & 4279 (17.6 \\%) & 24270 & nl & 6021 (25.4 \\%) & 5130 (21.6 \\%) & 4085 (17.2 \\%) & 5465 (23.1 \\%) & 2996 (12.6 \\%) & 23697 \\\\\n",
      "ko & 5964 (27.9 \\%) & 3509 (16.4 \\%) & 4009 (18.7 \\%) & 5425 (25.4 \\%) & 2491 (11.6 \\%) & 21398 & ca & 4506 (24.5 \\%) & 2425 (13.2 \\%) & 1518 (8.2 \\%) & 6783 (36.8 \\%) & 3178 (17.3 \\%) & 18410 \\\\\n",
      "fa & 7615 (42.1 \\%) & 3447 (19.0 \\%) & 1347 (7.4 \\%) & 3571 (19.7 \\%) & 2127 (11.7 \\%) & 18107 & ar & 7229 (40.5 \\%) & 2661 (14.9 \\%) & 1670 (9.4 \\%) & 3996 (22.4 \\%) & 2304 (12.9 \\%) & 17860 \\\\\n",
      "tr & 4474 (29.9 \\%) & 2427 (16.2 \\%) & 1652 (11.0 \\%) & 3983 (26.6 \\%) & 2449 (16.3 \\%) & 14985 & fi & 5187 (37.8 \\%) & 2162 (15.7 \\%) & 1924 (14.0 \\%) & 2907 (21.2 \\%) & 1548 (11.3 \\%) & 13728 \\\\\n",
      "id & 3267 (23.8 \\%) & 2216 (16.1 \\%) & 1749 (12.7 \\%) & 4609 (33.6 \\%) & 1886 (13.7 \\%) & 13727 & cs & 3687 (30.0 \\%) & 1990 (16.2 \\%) & 1556 (12.6 \\%) & 3201 (26.0 \\%) & 1870 (15.2 \\%) & 12304 \\\\\n",
      "vi & 3797 (31.7 \\%) & 1562 (13.0 \\%) & 999 (8.3 \\%) & 4117 (34.4 \\%) & 1510 (12.6 \\%) & 11985 & sv & 3453 (32.2 \\%) & 1978 (18.4 \\%) & 1391 (13.0 \\%) & 2610 (24.3 \\%) & 1297 (12.1 \\%) & 10729 \\\\\n",
      "hu & 2422 (31.0 \\%) & 1193 (15.3 \\%) & 1013 (13.0 \\%) & 2467 (31.6 \\%) & 708 (9.1 \\%) & 7803 & no & 1673 (23.0 \\%) & 1053 (14.5 \\%) & 1048 (14.4 \\%) & 2543 (34.9 \\%) & 966 (13.3 \\%) & 7283 \\\\\n",
      "el & 3032 (45.0 \\%) & 824 (12.2 \\%) & 445 (6.6 \\%) & 1937 (28.7 \\%) & 505 (7.5 \\%) & 6743 & az & 1982 (31.4 \\%) & 703 (11.1 \\%) & 666 (10.5 \\%) & 2586 (41.0 \\%) & 376 (6.0 \\%) & 6313 \\\\\n",
      "eo & 1218 (20.9 \\%) & 2237 (38.3 \\%) & 576 (9.9 \\%) & 1045 (17.9 \\%) & 762 (13.1 \\%) & 5838 & sr & 2251 (41.4 \\%) & 735 (13.5 \\%) & 514 (9.5 \\%) & 1310 (24.1 \\%) & 627 (11.5 \\%) & 5437 \\\\\n",
      "ur & 3941 (74.8 \\%) & 666 (12.6 \\%) & 157 (3.0 \\%) & 200 (3.8 \\%) & 304 (5.8 \\%) & 5268 & eu & 1047 (20.4 \\%) & 434 (8.4 \\%) & 1515 (29.5 \\%) & 1543 (30.0 \\%) & 603 (11.7 \\%) & 5142 \\\\\n",
      "bg & 1546 (32.2 \\%) & 752 (15.6 \\%) & 400 (8.3 \\%) & 1578 (32.8 \\%) & 531 (11.0 \\%) & 4807 & et & 1172 (24.9 \\%) & 857 (18.2 \\%) & 813 (17.3 \\%) & 1201 (25.5 \\%) & 662 (14.1 \\%) & 4705 \\\\\n",
      "uz & 2631 (60.7 \\%) & 318 (7.3 \\%) & 172 (4.0 \\%) & 527 (12.1 \\%) & 690 (15.9 \\%) & 4338 & ro & 1103 (25.7 \\%) & 546 (12.7 \\%) & 779 (18.2 \\%) & 1364 (31.8 \\%) & 495 (11.5 \\%) & 4287 \\\\\n",
      "gl & 1030 (24.2 \\%) & 461 (10.8 \\%) & 307 (7.2 \\%) & 1847 (43.4 \\%) & 613 (14.4 \\%) & 4258 & simple & 1344 (35.1 \\%) & 621 (16.2 \\%) & 600 (15.7 \\%) & 734 (19.2 \\%) & 532 (13.9 \\%) & 3831 \\\\\n",
      "hy & 1354 (35.6 \\%) & 1007 (26.5 \\%) & 387 (10.2 \\%) & 625 (16.5 \\%) & 426 (11.2 \\%) & 3799 & hr & 1107 (32.1 \\%) & 531 (15.4 \\%) & 220 (6.4 \\%) & 719 (20.8 \\%) & 876 (25.4 \\%) & 3453 \\\\\n",
      "sk & 1028 (32.9 \\%) & 497 (15.9 \\%) & 384 (12.3 \\%) & 782 (25.0 \\%) & 432 (13.8 \\%) & 3123 & be & 839 (27.1 \\%) & 386 (12.5 \\%) & 554 (17.9 \\%) & 876 (28.3 \\%) & 437 (14.1 \\%) & 3092 \\\\\n",
      "ms & 897 (32.7 \\%) & 710 (25.9 \\%) & 298 (10.9 \\%) & 414 (15.1 \\%) & 427 (15.5 \\%) & 2746 & lt & 497 (20.6 \\%) & 372 (15.4 \\%) & 450 (18.6 \\%) & 606 (25.1 \\%) & 488 (20.2 \\%) & 2413 \\\\\n",
      "da & 679 (28.7 \\%) & 369 (15.6 \\%) & 298 (12.6 \\%) & 619 (26.2 \\%) & 398 (16.8 \\%) & 2363 & hi & 1002 (42.8 \\%) & 721 (30.8 \\%) & 84 (3.6 \\%) & 156 (6.7 \\%) & 379 (16.2 \\%) & 2342 \\\\\n",
      "lv & 541 (27.7 \\%) & 328 (16.8 \\%) & 229 (11.7 \\%) & 500 (25.6 \\%) & 358 (18.3 \\%) & 1956 & oc & 552 (28.9 \\%) & 210 (11.0 \\%) & 164 (8.6 \\%) & 857 (44.8 \\%) & 129 (6.7 \\%) & 1912 \\\\\n",
      "ka & 1106 (61.9 \\%) & 233 (13.0 \\%) & 87 (4.9 \\%) & 222 (12.4 \\%) & 138 (7.7 \\%) & 1786 & kk & 942 (59.1 \\%) & 309 (19.4 \\%) & 85 (5.3 \\%) & 140 (8.8 \\%) & 119 (7.5 \\%) & 1595 \\\\\n",
      "sl & 409 (26.5 \\%) & 273 (17.7 \\%) & 123 (8.0 \\%) & 494 (32.0 \\%) & 245 (15.9 \\%) & 1544 & af & 695 (48.0 \\%) & 104 (7.2 \\%) & 161 (11.1 \\%) & 262 (18.1 \\%) & 227 (15.7 \\%) & 1449 \\\\\n",
      "la & 357 (29.0 \\%) & 266 (21.6 \\%) & 220 (17.9 \\%) & 315 (25.6 \\%) & 71 (5.8 \\%) & 1229 & mg & 406 (33.7 \\%) & 164 (13.6 \\%) & 55 (4.6 \\%) & 157 (13.1 \\%) & 421 (35.0 \\%) & 1203 \\\\\n",
      "te & 260 (22.4 \\%) & 158 (13.6 \\%) & 95 (8.2 \\%) & 314 (27.1 \\%) & 333 (28.7 \\%) & 1160 & ta & 362 (31.6 \\%) & 205 (17.9 \\%) & 67 (5.8 \\%) & 315 (27.5 \\%) & 197 (17.2 \\%) & 1146 \\\\\n",
      "is & 158 (15.1 \\%) & 75 (7.2 \\%) & 144 (13.8 \\%) & 559 (53.5 \\%) & 109 (10.4 \\%) & 1045 & bs & 352 (36.3 \\%) & 54 (5.6 \\%) & 144 (14.9 \\%) & 257 (26.5 \\%) & 162 (16.7 \\%) & 969 \\\\\n",
      "mk & 331 (35.9 \\%) & 157 (17.0 \\%) & 120 (13.0 \\%) & 202 (21.9 \\%) & 113 (12.2 \\%) & 923 & br & 222 (26.1 \\%) & 197 (23.1 \\%) & 116 (13.6 \\%) & 198 (23.2 \\%) & 119 (14.0 \\%) & 852 \\\\\n",
      "sh & 217 (26.9 \\%) & 107 (13.3 \\%) & 94 (11.6 \\%) & 279 (34.6 \\%) & 110 (13.6 \\%) & 807 & lb & 184 (24.4 \\%) & 148 (19.6 \\%) & 168 (22.3 \\%) & 117 (15.5 \\%) & 137 (18.2 \\%) & 754 \\\\\n",
      "ba & 212 (32.7 \\%) & 60 (9.2 \\%) & 21 (3.2 \\%) & 206 (31.7 \\%) & 150 (23.1 \\%) & 649 & ku & 263 (42.8 \\%) & 130 (21.2 \\%) & 31 (5.0 \\%) & 163 (26.5 \\%) & 27 (4.4 \\%) & 614 \\\\\n",
      "fy & 187 (32.3 \\%) & 83 (14.3 \\%) & 50 (8.6 \\%) & 183 (31.6 \\%) & 76 (13.1 \\%) & 579 & sw & 173 (31.0 \\%) & 200 (35.8 \\%) & 100 (17.9 \\%) & 54 (9.7 \\%) & 31 (5.6 \\%) & 558 \\\\\n",
      "sq & 128 (24.5 \\%) & 77 (14.7 \\%) & 85 (16.3 \\%) & 131 (25.0 \\%) & 102 (19.5 \\%) & 523 & kn & 315 (61.3 \\%) & 27 (5.3 \\%) & 15 (2.9 \\%) & 83 (16.1 \\%) & 74 (14.4 \\%) & 514 \\\\\n",
      "tl & 115 (22.5 \\%) & 114 (22.3 \\%) & 18 (3.5 \\%) & 144 (28.1 \\%) & 121 (23.6 \\%) & 512 & mn & 250 (53.5 \\%) & 68 (14.6 \\%) & 26 (5.6 \\%) & 81 (17.3 \\%) & 42 (9.0 \\%) & 467 \\\\\n",
      "as & 214 (46.6 \\%) & 77 (16.8 \\%) & 21 (4.6 \\%) & 54 (11.8 \\%) & 93 (20.3 \\%) & 459 & cy & 224 (58.0 \\%) & 56 (14.5 \\%) & 41 (10.6 \\%) & 44 (11.4 \\%) & 21 (5.4 \\%) & 386 \\\\\n",
      "cv & 72 (23.7 \\%) & 67 (22.0 \\%) & 51 (16.8 \\%) & 74 (24.3 \\%) & 40 (13.2 \\%) & 304 & ha & 201 (76.7 \\%) & 11 (4.2 \\%) & 4 (1.5 \\%) & 17 (6.5 \\%) & 29 (11.1 \\%) & 262 \\\\\n",
      "ne & 100 (40.0 \\%) & 94 (37.6 \\%) & 5 (2.0 \\%) & 10 (4.0 \\%) & 41 (16.4 \\%) & 250 & io & 14 (6.1 \\%) & 46 (20.0 \\%) & 65 (28.3 \\%) & 80 (34.8 \\%) & 25 (10.9 \\%) & 230 \\\\\n",
      "ga & 61 (29.9 \\%) & 34 (16.7 \\%) & 18 (8.8 \\%) & 79 (38.7 \\%) & 12 (5.9 \\%) & 204 & jv & 66 (41.0 \\%) & 21 (13.0 \\%) & 3 (1.9 \\%) & 46 (28.6 \\%) & 25 (15.5 \\%) & 161 \\\\\n",
      "pa & 23 (16.5 \\%) & 40 (28.8 \\%) & 39 (28.1 \\%) & 13 (9.4 \\%) & 24 (17.3 \\%) & 139 & ary & 43 (33.3 \\%) & 29 (22.5 \\%) & 14 (10.9 \\%) & 32 (24.8 \\%) & 11 (8.5 \\%) & 129 \\\\\n",
      "tg & 51 (41.5 \\%) & 38 (30.9 \\%) & 6 (4.9 \\%) & 21 (17.1 \\%) & 7 (5.7 \\%) & 123 & km & 62 (65.3 \\%) & 22 (23.2 \\%) & 3 (3.2 \\%) & 4 (4.2 \\%) & 4 (4.2 \\%) & 95 \\\\\n",
      "tt & 17 (18.1 \\%) & 13 (13.8 \\%) & 35 (37.2 \\%) & 9 (9.6 \\%) & 20 (21.3 \\%) & 94 & an & 8 (11.4 \\%) & 6 (8.6 \\%) & 13 (18.6 \\%) & 18 (25.7 \\%) & 25 (35.7 \\%) & 70 \\\\\n",
      "mr & 46 (68.7 \\%) & 10 (14.9 \\%) & 6 (9.0 \\%) & 4 (6.0 \\%) & 1 (1.5 \\%) & 67 & so & 29 (48.3 \\%) & 10 (16.7 \\%) & 3 (5.0 \\%) & 17 (28.3 \\%) & 1 (1.7 \\%) & 60 \\\\\n",
      "ast & 14 (28.6 \\%) & 22 (44.9 \\%) & 1 (2.0 \\%) & 11 (22.4 \\%) & 1 (2.0 \\%) & 49 & ce & 0 (0.0 \\%) & 2 (4.2 \\%) & 0 (0.0 \\%) & 45 (93.8 \\%) & 1 (2.1 \\%) & 48 \\\\\n",
      "sco & 18 (45.0 \\%) & 15 (37.5 \\%) & 3 (7.5 \\%) & 2 (5.0 \\%) & 2 (5.0 \\%) & 40 & si & 9 (25.0 \\%) & 3 (8.3 \\%) & 3 (8.3 \\%) & 16 (44.4 \\%) & 5 (13.9 \\%) & 36 \\\\\n",
      "gu & 18 (62.1 \\%) & 5 (17.2 \\%) & 0 (0.0 \\%) & 4 (13.8 \\%) & 2 (6.9 \\%) & 29 & lmo & 3 (11.5 \\%) & 2 (7.7 \\%) & 1 (3.8 \\%) & 1 (3.8 \\%) & 19 (73.1 \\%) & 26 \\\\\n",
      "azb & 15 (68.2 \\%) & 3 (13.6 \\%) & 4 (18.2 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 22 & bar & 9 (42.9 \\%) & 2 (9.5 \\%) & 2 (9.5 \\%) & 7 (33.3 \\%) & 1 (4.8 \\%) & 21 \\\\\n",
      "min & 11 (52.4 \\%) & 2 (9.5 \\%) & 0 (0.0 \\%) & 1 (4.8 \\%) & 7 (33.3 \\%) & 21 & yi & 12 (57.1 \\%) & 2 (9.5 \\%) & 1 (4.8 \\%) & 6 (28.6 \\%) & 0 (0.0 \\%) & 21 \\\\\n",
      "om & 13 (72.2 \\%) & 0 (0.0 \\%) & 1 (5.6 \\%) & 3 (16.7 \\%) & 1 (5.6 \\%) & 18 & sa & 4 (22.2 \\%) & 5 (27.8 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 9 (50.0 \\%) & 18 \\\\\n",
      "gd & 4 (28.6 \\%) & 2 (14.3 \\%) & 5 (35.7 \\%) & 3 (21.4 \\%) & 0 (0.0 \\%) & 14 & sd & 8 (57.1 \\%) & 1 (7.1 \\%) & 3 (21.4 \\%) & 2 (14.3 \\%) & 0 (0.0 \\%) & 14 \\\\\n",
      "am & 9 (75.0 \\%) & 3 (25.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 12 & lo & 4 (36.4 \\%) & 3 (27.3 \\%) & 3 (27.3 \\%) & 1 (9.1 \\%) & 0 (0.0 \\%) & 11 \\\\\n",
      "ps & 5 (50.0 \\%) & 5 (50.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 10 & vo & 0 (0.0 \\%) & 7 (100.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 7 \\\\\n",
      "su & 3 (50.0 \\%) & 0 (0.0 \\%) & 1 (16.7 \\%) & 2 (33.3 \\%) & 0 (0.0 \\%) & 6 & scn & 0 (0.0 \\%) & 3 (60.0 \\%) & 2 (40.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 5 \\\\\n",
      "ug & 0 (0.0 \\%) & 1 (100.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 1 & xh & 1 (100.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 0 (0.0 \\%) & 1 \\\\\n",
      "nds-nl & 0 & 0 & 0 & 0 & 0 & 0 & pms & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "uk & 0 & 0 & 0 & 0 & 0 & 0 & war & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n"
     ]
    }
   ],
   "source": [
    "same_line = True\n",
    "for stat in stats:\n",
    "    if stat[0] == 'ru' or stat[0] == 'de':\n",
    "        continue\n",
    "    print(stat[0] + ' & ', end='')\n",
    "    total_links = sum(stat[1].values())\n",
    "    if total_links == 0:\n",
    "        print('0 & 0 & 0 & 0 & 0 & 0', end='')\n",
    "    else:\n",
    "        for key in ['text_present', 'missing_mention', 'missing_sentence', 'missing_span', 'missing_section']:\n",
    "            print(f'{stat[1][key]} ({stat[1][key] / total_links * 100:.1f} \\\\%) & ', end='')\n",
    "        print(total_links, end='')\n",
    "    if same_line:\n",
    "        print(' & ', end='')\n",
    "        same_line = False\n",
    "    else:\n",
    "        print(' \\\\\\\\')\n",
    "        same_line = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_present 2364.9 29.653343850845864\n",
      "missing_mention 1533.8 19.23222918450141\n",
      "missing_sentence 1188.3545454545454 14.900708678201733\n",
      "missing_span 1999.6727272727273 25.073780274420443\n",
      "missing_section 888.4272727272727 11.139938012030544\n",
      "total 7975.154545454546 100.0\n"
     ]
    }
   ],
   "source": [
    "micro_avg = {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0, 'total': 0}\n",
    "for stat in stats:\n",
    "    for key in ['text_present', 'missing_mention', 'missing_sentence', 'missing_span', 'missing_section']:\n",
    "        micro_avg[key] += stat[1][key]\n",
    "    micro_avg['total'] += sum(stat[1].values())\n",
    "for key in ['text_present', 'missing_mention', 'missing_sentence', 'missing_span', 'missing_section', 'total']:\n",
    "    micro_avg[key] /= len(stats)\n",
    "for key in micro_avg:\n",
    "    print(key, micro_avg[key], micro_avg[key] / micro_avg['total'] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stat \u001b[38;5;129;01min\u001b[39;00m stats:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_present\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_mention\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_span\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_section\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m         macro_avg[key] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mstat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      5\u001b[0m     macro_avg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_present\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_mention\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_span\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_section\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "macro_avg = {'text_present': 0, 'missing_mention': 0, 'missing_sentence': 0, 'missing_span': 0, 'missing_section': 0, 'total': 0}\n",
    "for stat in stats:\n",
    "    for key in ['text_present', 'missing_mention', 'missing_sentence', 'missing_span', 'missing_section']:\n",
    "        if sum(stat[1].values()) == 0:\n",
    "            macro_avg[key] += 0\n",
    "        else:\n",
    "            macro_avg[key] += stat[1][key] / sum(stat[1].values()) * 100\n",
    "    macro_avg['total'] += 100\n",
    "for key in ['text_present', 'missing_mention', 'missing_sentence', 'missing_span', 'missing_section', 'total']:\n",
    "    macro_avg[key] /= len(stats)\n",
    "for key in macro_avg:\n",
    "    print(key, macro_avg[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_dump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
