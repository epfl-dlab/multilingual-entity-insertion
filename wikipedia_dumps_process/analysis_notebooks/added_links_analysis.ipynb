{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pages = pd.concat([pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20230901/processed_data/good_pages_0.parquet', columns=['title', 'lead_paragraph']),\n",
    "                      pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20230901/processed_data/good_pages_1.parquet', columns=['title', 'lead_paragraph']),\n",
    "                      pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20231001/processed_data/good_pages_0.parquet', columns=['title', 'lead_paragraph']),\n",
    "                      pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20231001/processed_data/good_pages_1.parquet', columns=['title', 'lead_paragraph'])]).reset_index(drop=True)\n",
    "df_pages = df_pages.drop_duplicates(\n",
    "    subset=['title']).reset_index(drop=True)\n",
    "df_pages = df_pages.to_dict(orient='records')\n",
    "page_leads = {row['title']: row['lead_paragraph'] for row in df_pages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20230901/eval/test_data.parquet')\n",
    "df['language'] = 'simple'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(df)} links in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning links')\n",
    "print(f'We started with {len(df)} links')\n",
    "no_context = df['context'] == ''\n",
    "print(f'There are {no_context.sum()} links with no context')\n",
    "no_neg_contexts = df['negative_contexts'] == '[]'\n",
    "print(f'There are {no_neg_contexts.sum()} links with no negative contexts')\n",
    "missing_page = ~df['target_title'].isin(page_leads)\n",
    "print(f'There are {missing_page.sum()} links with missing pages')\n",
    "missing_section = df['missing_category'] == 'missing_section'\n",
    "print(f'There are {missing_section.sum()} links with missing sections')\n",
    "df_clean = df[~no_context & ~no_neg_contexts & ~missing_page & ~missing_section]\n",
    "print(f\"After cleaning, there are {len(df_clean)} links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    sample = df[df['context'] == ''].sample(1).to_dict(orient='records')[0]\n",
    "    for key in sample:\n",
    "        if key != 'negative_contexts':\n",
    "            print(key, sample[key])\n",
    "    print('#############')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib stacked bar plot\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.set_title('Link Insertion Strategy Distribution')\n",
    "ax.set_ylabel('Percentage of Links')\n",
    "ax.set_xlabel('Language')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_xticks(np.arange(len(df['language'].unique())+2))\n",
    "ax.set_xticklabels([''] + df['language'].unique().tolist() + [''], rotation=0)\n",
    "\n",
    "present_text = [0]\n",
    "missing_mention = [0]\n",
    "missing_sentence = [0]\n",
    "missing_span = [0]\n",
    "missing_section = [0]\n",
    "\n",
    "df = df[df['context'] != '']\n",
    "for language in df['language'].unique():\n",
    "    present_text.append(len(df[(df['language'] == language) & (df['missing_category'].isna())]) / len(df[df['language'] == language]) * 100)\n",
    "    missing_mention.append(len(df[(df['language'] == language) & (df['missing_category'] == 'missing_mention')]) / len(df[df['language'] == language]) * 100)\n",
    "    missing_sentence.append(len(df[(df['language'] == language) & (df['missing_category'] == 'missing_sentence')]) / len(df[df['language'] == language]) * 100)\n",
    "    missing_span.append(len(df[(df['language'] == language) & (df['missing_category'] == 'missing_span')]) / len(df[df['language'] == language]) * 100)\n",
    "    missing_section.append(len(df[(df['language'] == language) & (df['missing_category'] == 'missing_section')]) / len(df[df['language'] == language]) * 100)\n",
    "\n",
    "present_text.append(0)\n",
    "missing_mention.append(0)\n",
    "missing_sentence.append(0)\n",
    "missing_span.append(0)\n",
    "missing_section.append(0)\n",
    "    \n",
    "bottom = np.zeros(len(df['language'].unique())+2)\n",
    "present_text = np.array(present_text)\n",
    "missing_mention = np.array(missing_mention)\n",
    "missing_sentence = np.array(missing_sentence)\n",
    "missing_span = np.array(missing_span)\n",
    "missing_section = np.array(missing_section)\n",
    "\n",
    "ax.bar(np.arange(len(df['language'].unique())+2), present_text, label='Text Present', bottom=bottom)\n",
    "bottom += present_text\n",
    "ax.bar(np.arange(len(df['language'].unique())+2), missing_mention, label='Mention Missing', bottom=bottom)\n",
    "bottom += missing_mention\n",
    "ax.bar(np.arange(len(df['language'].unique())+2), missing_sentence, label='Sentence Missing', bottom=bottom)\n",
    "bottom += missing_sentence\n",
    "ax.bar(np.arange(len(df['language'].unique())+2), missing_span, label='Span Missing', bottom=bottom)\n",
    "bottom += missing_span\n",
    "ax.bar(np.arange(len(df['language'].unique())+2), missing_section, label='Section Missing', bottom=bottom)\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "# flip order of legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "# add labels inside each bar\n",
    "for i, v in enumerate(present_text):\n",
    "    if v > 0:\n",
    "        ax.text(i, v/2, f'{v:.2f}%', color='white', ha='center', va='center', fontweight='bold')\n",
    "for i, v in enumerate(missing_mention):\n",
    "    if v > 0:\n",
    "        ax.text(i, present_text[i]+v/2, f'{v:.2f}%', color='white', ha='center', va='center', fontweight='bold')\n",
    "for i, v in enumerate(missing_sentence):\n",
    "    if v > 0:\n",
    "        ax.text(i, present_text[i]+missing_mention[i]+v/2, f'{v:.2f}%', color='white', ha='center', va='center', fontweight='bold')\n",
    "for i, v in enumerate(missing_span):\n",
    "    if v > 0:\n",
    "        ax.text(i, present_text[i]+missing_mention[i]+missing_sentence[i]+v/2, f'{v:.2f}%', color='white', ha='center', va='center', fontweight='bold')\n",
    "for i, v in enumerate(missing_section):\n",
    "    if v > 0:\n",
    "        ax.text(i, present_text[i]+missing_mention[i]+missing_sentence[i]+missing_span[i]+v/2, f'{v:.2f}%', color='white', ha='center', va='center', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram with the number of negative contexts per link\n",
    "negative_contexts = df[df['context'] != '']['negative_contexts'].reset_index(drop=True).apply(literal_eval).apply(len)\n",
    "negative_contexts.hist(bins=100)\n",
    "# set log x axis\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of negative contexts')\n",
    "plt.ylabel('Number of links')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_contexts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze processing failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "def process_title(title):\n",
    "    return urllib.parse.unquote(title).replace('_', ' ')\n",
    "\n",
    "def update_targets(target_name, redirect_map):\n",
    "    counter = 0\n",
    "    while target_name in redirect_map:\n",
    "        target_name = redirect_map[target_name]\n",
    "        counter += 1\n",
    "        if counter > 10:\n",
    "            break\n",
    "    return target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat([pd.read_parquet(f'/scratch/tsoares/wikidumps/simplewiki-NS0-20231001/processed_data/good_links_{i}.parquet') for i in range(2)]).reset_index(drop=True)\n",
    "df_2 = pd.concat([pd.read_parquet(f'/scratch/tsoares/wikidumps/simplewiki-NS0-20231101/processed_data/good_links_{i}.parquet') for i in range(2)]).reset_index(drop=True)\n",
    "# df_pages_1 = pd.concat([pd.read_parquet(f'/scratch/tsoares/wikidumps/simplewiki-NS0-20231001/processed_data/good_pages_{i}.parquet') for i in range(2)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_map = pd.concat([pd.read_parquet(f'/scratch/tsoares/wikidumps/simplewiki-NS0-2023{month}01/processed_data/redirect_map.parquet') for month in [10, 11]]).reset_index(drop=True)\n",
    "redirect_map = redirect_map['redirect'].to_dict()\n",
    "\n",
    "df_1['target_title'] = df_1['target_title'].apply(\n",
    "    lambda x: update_targets(x, redirect_map))\n",
    "df_2['target_title'] = df_2['target_title'].apply(\n",
    "    lambda x: update_targets(x, redirect_map))\n",
    "\n",
    "df_1 = df_1[['source_title', 'target_title',\n",
    "                'source_ID', 'target_ID', 'source_version']]\n",
    "df_2 = df_2[['source_title', 'target_title',\n",
    "                'source_ID', 'target_ID', 'source_version']]\n",
    "\n",
    "# group the links by source and target and count the number of links\n",
    "df_1 = df_1.groupby(['source_title', 'target_title', 'source_ID',\n",
    "                    'target_ID', 'source_version']).size().reset_index(name='count')\n",
    "df_2 = df_2.groupby(['source_title', 'target_title', 'source_ID',\n",
    "                    'target_ID', 'source_version']).size().reset_index(name='count')\n",
    "\n",
    "# find all new links added in df_2. Consider two cases\n",
    "# 1. The row is not present in df_1\n",
    "# 2. The row is present in df_1 but the count is smaller in df_1\n",
    "df_diff = df_2.merge(df_1, how='left', on=[\n",
    "                    'source_title', 'target_title', 'source_ID', 'target_ID'], suffixes=('_2', '_1'))\n",
    "df_diff = df_diff[(df_diff['count_1'].isna()) | (df_diff['count_2'] > df_diff['count_1'])]\n",
    "df_diff['count_1'] = df_diff['count_1'].fillna(0)\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].fillna('&oldid=0')\n",
    "df_diff['count'] = df_diff['count_2'] - df_diff['count_1']\n",
    "df_diff = df_diff[['source_title', 'target_title', 'source_ID',\n",
    "                'target_ID', 'source_version_1', 'source_version_2', 'count']]\n",
    "\n",
    "\n",
    "initial_size = df_diff['count'].sum()\n",
    "print(f'Initially, there are {df_diff[\"count\"].sum()} new candidate links, from {len(df_diff)} unique src-tgt pairs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff['source_ID'] = df_diff['source_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare links in df with links in df_diff\n",
    "# find links with source title in df_diff and not in df\n",
    "df_diff['source_title'] = df_diff['source_title'].apply(process_title)\n",
    "df['source_title'] = df['source_title'].apply(process_title)\n",
    "df_diff_2 = df_diff.merge(df[['source_title', 'source_ID']].drop_duplicates(), how='left', on='source_title', suffixes=('_diff', '_df'))\n",
    "df_diff_2 = df_diff_2[df_diff_2['source_ID_df'].isna()]\n",
    "df_diff_2 = df_diff_2[['source_title', 'target_title', 'source_ID_diff', 'target_ID', 'source_version_1', 'source_version_2', 'count']]\n",
    "df_diff_2['source_ID'] = df_diff_2['source_ID_diff']\n",
    "df_diff_2 = df_diff_2.drop(columns=['source_ID_diff'])\n",
    "df_diff_2 = df_diff_2.rename(columns={'source_ID': 'source_ID_diff'})\n",
    "df_diff_2['source_ID_diff'] = df_diff_2['source_ID_diff'].astype(int)\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].apply(lambda x: x.replace('&oldid=', ''))\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].astype(int)\n",
    "# df_diff['source_version_2'] = df_diff['source_version_2'].apply(lambda x: x.replace('&oldid=', ''))\n",
    "# df_diff['source_version_2'] = df_diff['source_version_2'].astype(int)\n",
    "df_diff_2['count'] = df_diff_2['count'].astype(int)\n",
    "df_diff_2 = df_diff_2.sort_values(by=['source_ID_diff', 'target_ID', 'source_version_1', 'source_version_2']).reset_index(drop=True)\n",
    "df_diff_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff[df_diff['source_version_1'] != '&oldid=0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare links in df with links in df_diff\n",
    "# find links with target title in df_diff and not in df\n",
    "df_diff_3 = df_diff.merge(df[['target_title']].drop_duplicates(), how='left', on='target_title', suffixes=('_diff', '_df'))\n",
    "df_diff_3 = df_diff_3[['source_title', 'target_title', 'source_ID', 'source_version_1', 'source_version_2', 'count']]\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].apply(lambda x: x.replace('&oldid=', ''))\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].astype(int)\n",
    "# df_diff['source_version_2'] = df_diff['source_version_2'].apply(lambda x: x.replace('&oldid=', ''))\n",
    "# df_diff['source_version_2'] = df_diff['source_version_2'].astype(int)\n",
    "df_diff_3['count'] = df_diff_3['count'].astype(int)\n",
    "df_diff_3 = df_diff_3.sort_values(by=['source_ID', 'source_version_1', 'source_version_2']).reset_index(drop=True)\n",
    "df_diff_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_3[df_diff_3['source_version_1'] != '&oldid=0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare links in df with links in df_diff\n",
    "# find links with target title and source title in df_diff and not in df\n",
    "df_diff_4 = df_diff.merge(df[['source_title', 'target_title', 'source_ID']].drop_duplicates(), how='left', on=['source_title', 'target_title'], suffixes=('_diff', '_df'))\n",
    "df_diff_4 = df_diff_4[df_diff_4['source_ID_df'].isna()]\n",
    "df_diff_4 = df_diff_4[['source_title', 'target_title', 'source_ID_diff', 'target_ID', 'source_version_1', 'source_version_2', 'count']]\n",
    "df_diff_4['source_ID'] = df_diff_4['source_ID_diff']\n",
    "df_diff_4 = df_diff_4.drop(columns=['source_ID_diff'])\n",
    "df_diff_4 = df_diff_4.rename(columns={'source_ID': 'source_ID_diff'})\n",
    "df_diff_4['source_ID_diff'] = df_diff_4['source_ID_diff'].astype(int)\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].apply(lambda x: x.replace('&oldid=', ''))\n",
    "# df_diff['source_version_1'] = df_diff['source_version_1'].astype(int)\n",
    "# df_diff['source_version_2'] = df_diff['source_version_2'].apply(lambda x: x.replace('&oldid=', ''))\n",
    "# df_diff['source_version_2'] = df_diff['source_version_2'].astype(int)\n",
    "df_diff_4['count'] = df_diff_4['count'].astype(int)\n",
    "df_diff_4 = df_diff_4.sort_values(by=['source_ID_diff', 'target_ID', 'source_version_1', 'source_version_2']).reset_index(drop=True)\n",
    "df_diff_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_4[df_diff_4['source_version_1'] != '&oldid=0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['source_title'] == 'Acanthopterygii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff[df_diff['source_title'] == 'Acanthopterygii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions = pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20231001/eval/link_versions.parquet')\n",
    "df_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions[df_versions['source_title'] == 'Acanthopterygii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print df_diff sorted by source_ID\n",
    "# df_diff[df_diff['source_version_1']  != '&oldid=0'].sort_values(by=['source_ID']).reset_index(drop=True)\n",
    "df_diff.sort_values(by=['source_ID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_dump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
