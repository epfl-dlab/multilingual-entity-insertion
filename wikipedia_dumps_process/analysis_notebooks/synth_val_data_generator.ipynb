{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_1 = '/scratch/tsoares/wikidumps/simplewiki-NS0-20230901'\n",
    "month_2 = '/scratch/tsoares/wikidumps/simplewiki-NS0-20231001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data corruption splits\n",
    "no_mask = 0.4\n",
    "mask_sentence = 0.3\n",
    "mask_mention = 0.2\n",
    "mask_span = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_1 = glob(os.path.join(month_1, 'processed_data', \"links*\"))\n",
    "dfs = []\n",
    "for file in tqdm(files_1):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "df_1 = pd.concat(dfs)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2 = glob(os.path.join(month_2, 'processed_data', \"links*\"))\n",
    "dfs = []\n",
    "for file in tqdm(files_2):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "df_2 = pd.concat(dfs)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in tqdm(glob(os.path.join(month_2, 'processed_data', \"pages*\"))):\n",
    "    dfs.append(pd.read_parquet(file))\n",
    "df_pages = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data into better structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_1 = df_1.to_dict(orient='records')\n",
    "df_links_2 = df_2.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(df_links_1):\n",
    "    for key in row:\n",
    "        if 'index' in key and row[key] == row[key]:\n",
    "            row[key] = int(row[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(df_links_2):\n",
    "    for key in row:\n",
    "        if 'index' in key and row[key] == row[key]:\n",
    "            row[key] = int(row[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = {}\n",
    "for mod_link in tqdm(df_links_1):\n",
    "    if mod_link['source_title'] not in old_data:\n",
    "        old_data[mod_link['source_title']] = {}\n",
    "    if mod_link['target_title'] not in old_data[mod_link['source_title']]:\n",
    "        old_data[mod_link['source_title']][mod_link['target_title']] = []\n",
    "    old_data[mod_link['source_title']][mod_link['target_title']].append(mod_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "for mod_link in tqdm(df_links_2):\n",
    "    if mod_link['source_title'] not in new_data:\n",
    "        new_data[mod_link['source_title']] = {}\n",
    "    if mod_link['target_title'] not in new_data[mod_link['source_title']]:\n",
    "        new_data[mod_link['source_title']][mod_link['target_title']] = []\n",
    "    new_data[mod_link['source_title']][mod_link['target_title']].append(mod_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_html = set(df_pages[(df_pages['HTML'].isna()) | (df_pages['HTML'] == '')]['title'].tolist())\n",
    "no_lead = set(df_pages[(df_pages['lead_paragraph'].isna()) | (df_pages['lead_paragraph'] == '')]['title'].tolist())\n",
    "short_lead = set(df_pages[df_pages['lead_paragraph'].apply(lambda x: x is not None and len(x.split()) < 6)]['title'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find added links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pages = 0\n",
    "new_page_links = 0\n",
    "new_links = []\n",
    "no_id_found = 0\n",
    "no_id_not_found = 0\n",
    "\n",
    "for source_page in tqdm(new_data):\n",
    "    if source_page not in old_data:\n",
    "        new_pages += 1\n",
    "        new_page_links += len(new_data[source_page])\n",
    "        continue\n",
    "    for target_page in new_data[source_page]:\n",
    "        if target_page not in old_data[source_page]:\n",
    "            new_links.extend(new_data[source_page][target_page])\n",
    "        else:\n",
    "            links_with_id = []\n",
    "            links_without_id = []\n",
    "            for mod_link in new_data[source_page][target_page]:\n",
    "                if mod_link['link_ID'] is None:\n",
    "                    links_without_id.append(mod_link)\n",
    "                else:\n",
    "                    links_with_id.append(mod_link)\n",
    "            \n",
    "            for mod_link in links_with_id:\n",
    "                found = False\n",
    "                for old_link in old_data[source_page][target_page]:\n",
    "                    if mod_link['link_ID'] == old_link['link_ID']:\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    new_links.append(mod_link)\n",
    "            \n",
    "            # try to find matches in the links without ID\n",
    "            used = set([])\n",
    "            for mod_link in links_without_id:\n",
    "                found = False\n",
    "                for i, old_link in enumerate(old_data[source_page][target_page]):\n",
    "                    if old_link['link_ID'] is None and old_link['mention'] == mod_link['mention'] and old_link['source_section'] == mod_link['source_section'] and i not in used:\n",
    "                        used.add(i)\n",
    "                        found = True\n",
    "                        no_id_found += 1\n",
    "                        break\n",
    "                if not found:\n",
    "                    no_id_not_found += 1\n",
    "                    new_links.append(mod_link)\n",
    "\n",
    "print(f\"The new data has {new_pages} new pages and {new_page_links} in these new pages\")\n",
    "print(f\"There are {len(new_links)} new links in the new data\")\n",
    "print(f\"From the links without ID, {no_id_found} ({no_id_found / (no_id_found + no_id_not_found) * 100:.2f}%) were matched to old links, and {no_id_not_found} ({no_id_not_found / (no_id_found + no_id_not_found) * 100:.2f}%) were not matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in new_links[:10]:\n",
    "    print(link['context'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean new links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_links = []\n",
    "for link in tqdm(new_links):\n",
    "    if link['target_ID'] is None:\n",
    "        continue\n",
    "    if link['source_QID'] is None:\n",
    "        continue\n",
    "    if link['source_title'] in no_lead:\n",
    "        continue\n",
    "    if link['source_title'] in short_lead:\n",
    "        continue\n",
    "    if link['target_QID'] is None:\n",
    "        continue\n",
    "    if link['target_title'] in no_html:\n",
    "        continue\n",
    "    if link['target_title'] in no_lead:\n",
    "        continue\n",
    "    if link['target_title'] in short_lead:\n",
    "        continue\n",
    "    if link['target_title'] == mod_link['source_title']:\n",
    "        continue\n",
    "    if link['context'] is None:\n",
    "        continue\n",
    "    link['source_title'] = urllib.parse.unquote(link['source_title']).replace('_', ' ')\n",
    "    link['target_title'] = urllib.parse.unquote(link['target_title']).replace('_', ' ')\n",
    "    link['context'] = \"\\n\".join(line for line in link['context'].split(\"\\n\") if line.strip() != '')\n",
    "    clean_links.append(link)\n",
    "\n",
    "print(f\"Out of the {len(new_links)} new links, {len(clean_links)} ({len(clean_links) / len(new_links) * 100:.2f}%) are valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in clean_links[:10]:\n",
    "    print(link['context'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply corruption to new links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link triage\n",
    "\n",
    "Let's find which links can handle each corruption. The corruptions are narrower and narrower in the following order no mask > mask mention > mask sentence > mask span. This means that any context which can handle \"mask sentence\" can also handle \"mask mention\" and \"no mask\", but not necessarily \"mask span\". All contexts can handle \"no mask\" (no corruption)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_span_links = []\n",
    "mask_mention_links = []\n",
    "mask_sentence_links = []\n",
    "no_mask_links = []\n",
    "for link in clean_links:\n",
    "    # mask span\n",
    "    if (link['context'][:link['context_span_start_index']] + link['context'][link['context_span_end_index']:]).strip() != '':\n",
    "        if link['context_span_start_index'] <= link['context_sentence_start_index'] and link['context_span_end_index'] >= link['context_sentence_end_index']:\n",
    "            mask_span_links.append(link)\n",
    "            continue\n",
    "    \n",
    "    # mask sentence\n",
    "    if (link['context'][:link['context_sentence_start_index']] + link['context'][link['context_sentence_end_index']:]).strip() != '':\n",
    "        if link['context_sentence_start_index'] <= link['context_mention_start_index'] and link['context_sentence_end_index'] > link['context_mention_end_index'] + 1:\n",
    "            mask_sentence_links.append(link)\n",
    "            continue\n",
    "    \n",
    "    # mask mention\n",
    "    if (link['context'][:link['context_mention_start_index']] + link['context'][link['context_mention_end_index']:]).strip() != '':\n",
    "        mask_mention_links.append(link)\n",
    "        continue\n",
    "    \n",
    "    # no mask\n",
    "    no_mask_links.append(link)\n",
    "    \n",
    "print(f\"Out of the {len(clean_links)} clean links, we got the following results:\")\n",
    "print(f\"\\t- Mask span: {len(mask_span_links)} ({len(mask_span_links) / len(clean_links) * 100:.2f}%)\")\n",
    "print(f\"\\t- Mask sentence: {len(mask_sentence_links)} ({len(mask_sentence_links) / len(clean_links) * 100:.2f}%)\")\n",
    "print(f\"\\t- Mask mention: {len(mask_mention_links)} ({len(mask_mention_links) / len(clean_links) * 100:.2f}%)\")\n",
    "print(f\"\\t- No mask: {len(no_mask_links)} ({len(no_mask_links) / len(clean_links) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the corrupted contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_links = []\n",
    "# shuffle mask span links\n",
    "random.shuffle(mask_span_links)\n",
    "for link in mask_span_links[:int(len(clean_links) * mask_span)]:\n",
    "    mod_link = link.copy()\n",
    "    mod_link['original_context'] = mod_link['context']\n",
    "    mod_link['context'] = mod_link['context'][:int(mod_link['context_span_start_index'])] + mod_link['context'][int(mod_link['context_span_end_index']):]\n",
    "    mod_link['context'] = re.sub(' +', ' ', mod_link['context'])\n",
    "    mod_link['context'] = re.sub('\\n ', '\\n', mod_link['context'])\n",
    "    mod_link['context'] = re.sub('\\n+', '\\n', mod_link['context'])\n",
    "    mod_link['context'] = mod_link['context'].strip()\n",
    "    mod_link['noise_strategy'] = 'mask_span'\n",
    "    final_links.append(mod_link)\n",
    "\n",
    "mask_sentence_links.extend(mask_span_links[int(len(clean_links) * mask_span):])\n",
    "random.shuffle(mask_sentence_links)\n",
    "for link in mask_sentence_links[:int(len(clean_links) * mask_sentence)]:\n",
    "    mod_link = link.copy()\n",
    "    mod_link['original_context'] = mod_link['context']\n",
    "    mod_link['context'] = mod_link['context'][:int(mod_link['context_sentence_start_index'])] + mod_link['context'][int(mod_link['context_sentence_end_index']):]\n",
    "    mod_link['context'] = re.sub(' +', ' ', mod_link['context'])\n",
    "    mod_link['context'] = re.sub('\\n ', '\\n', mod_link['context'])\n",
    "    mod_link['context'] = mod_link['context'].strip()\n",
    "    mod_link['noise_strategy'] = 'mask_sentence'\n",
    "    final_links.append(mod_link)\n",
    "    \n",
    "mask_mention_links.extend(mask_sentence_links[int(len(clean_links) * mask_sentence):])\n",
    "random.shuffle(mask_mention_links)\n",
    "for link in mask_mention_links[:int(len(clean_links) * mask_mention)]:\n",
    "    mod_link = link.copy()\n",
    "    mod_link['original_context'] = mod_link['context']\n",
    "    mod_link['context'] = mod_link['context'][:int(mod_link['context_mention_start_index'])] + mod_link['context'][int(mod_link['context_mention_end_index']):]\n",
    "    mod_link['context'] = re.sub(' +', ' ', mod_link['context'])\n",
    "    mod_link['context'] = re.sub('\\n ', '\\n', mod_link['context'])\n",
    "    mod_link['context'] = mod_link['context'].strip()\n",
    "    mod_link['noise_strategy'] = 'mask_mention'\n",
    "    final_links.append(mod_link)\n",
    "    \n",
    "no_mask_links.extend(mask_mention_links[int(len(clean_links) * mask_mention):])\n",
    "random.shuffle(no_mask_links)\n",
    "for link in no_mask_links:\n",
    "    mod_link = link.copy()\n",
    "    mod_link['original_context'] = mod_link['context']\n",
    "    mod_link['context'] = re.sub(' +', ' ', mod_link['context'])\n",
    "    mod_link['context'] = re.sub('\\n ', '\\n', mod_link['context'])\n",
    "    mod_link['context'] = mod_link['context'].strip()\n",
    "    mod_link['noise_strategy'] = 'no_mask'\n",
    "    final_links.append(mod_link)\n",
    "    \n",
    "print('In the end, we have the following distribution:')\n",
    "print(f\"\\t- Mask span: {len([link for link in final_links if link['noise_strategy'] == 'mask_span'])} ({len([link for link in final_links if link['noise_strategy'] == 'mask_span']) / len(final_links) * 100:.2f}%)\")\n",
    "print(f\"\\t- Mask sentence: {len([link for link in final_links if link['noise_strategy'] == 'mask_sentence'])} ({len([link for link in final_links if link['noise_strategy'] == 'mask_sentence']) / len(final_links) * 100:.2f}%)\")\n",
    "print(f\"\\t- Mask mention: {len([link for link in final_links if link['noise_strategy'] == 'mask_mention'])} ({len([link for link in final_links if link['noise_strategy'] == 'mask_mention']) / len(final_links) * 100:.2f}%)\")\n",
    "print(f\"\\t- No mask: {len([link for link in final_links if link['noise_strategy'] == 'no_mask'])} ({len([link for link in final_links if link['noise_strategy'] == 'no_mask']) / len(final_links) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(final_links)\n",
    "for link in final_links[:10]:\n",
    "    print('MODIFIED CONTEXT')\n",
    "    print(link['context'])\n",
    "    print()\n",
    "    print('ORIGINAL CONTEXT')\n",
    "    print(link['original_context'])\n",
    "    print()\n",
    "    print('MENTION')\n",
    "    print(link['mention'])\n",
    "    print()\n",
    "    print('NOISE STRATEGY')\n",
    "    print(link['noise_strategy'])\n",
    "    print()\n",
    "    print('SOURCE TITLE')\n",
    "    print(link['source_title'])\n",
    "    print('CONTEXT MENTION START INDEX')\n",
    "    print(link['context_mention_start_index'])\n",
    "    print('CONTEXT MENTION END INDEX')\n",
    "    print(link['context_mention_end_index'])\n",
    "    print('CONTEXT SENTENCE START INDEX')\n",
    "    print(link['context_sentence_start_index'])\n",
    "    print('CONTEXT SENTENCE END INDEX')\n",
    "    print(link['context_sentence_end_index'])\n",
    "    print('CONTEXT SPAN START INDEX')\n",
    "    print(link['context_span_start_index'])\n",
    "    print('CONTEXT SPAN END INDEX')\n",
    "    print(link['context_span_end_index'])\n",
    "    print('####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in clean_links[:10]:\n",
    "    for key in link:\n",
    "        print(key, link[key])\n",
    "    print('####################')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_dump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
