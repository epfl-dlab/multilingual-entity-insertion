{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, Comment, NavigableString, Tag\n",
    "import re\n",
    "import urllib\n",
    "import difflib\n",
    "from nltk import sent_tokenize\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_versions = pd.read_parquet('/scratch/tsoares/wikidumps/simplewiki-NS0-20230901/val_data/link_versions.parquet')\n",
    "df_versions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_versions.sample(1)\n",
    "source_title = sample['source_title'].iloc[0]\n",
    "source_ID = sample['source_ID'].iloc[0]\n",
    "first_version = sample['first_version'].iloc[0]\n",
    "second_version = sample['second_version'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = f'/scratch/tsoares/wikidumps/simplewiki-NS0-20230901/val_data/pages/{source_ID}_{first_version}.html'\n",
    "file_2 = f'/scratch/tsoares/wikidumps/simplewiki-NS0-20230901/val_data/pages/{source_ID}_{second_version}.html'\n",
    "with open(file_1, 'r') as f:\n",
    "    text_1 = f.read()\n",
    "with open(file_2, 'r') as f:\n",
    "    text_2 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_1 = BeautifulSoup(text_1, 'html.parser')\n",
    "html_2 = BeautifulSoup(text_2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_html(html):\n",
    "    # only keep the tag with class 'mw-parser-output'\n",
    "    html = html.find('div', {'class': 'mw-parser-output'})\n",
    "    # remove all figures, tables, captions, sup, style\n",
    "    # figures\n",
    "    for figure in html.find_all('figure'):\n",
    "        figure.decompose()\n",
    "    # tables\n",
    "    for table in html.find_all('table'):\n",
    "        table.decompose()\n",
    "    # captions\n",
    "    for caption in html.find_all('caption'):\n",
    "        caption.decompose()\n",
    "    # sup\n",
    "    for sup in html.find_all('sup'):\n",
    "        sup.decompose()\n",
    "    # style\n",
    "    for style in html.find_all('style'):\n",
    "        style.decompose()\n",
    "    \n",
    "    # remove all comments\n",
    "    comments = html.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    [comment.extract() for comment in comments]\n",
    "    \n",
    "    # remove all tags with class 'mw-editsection'\n",
    "    for tag in html.find_all('span', {'class': 'mw-editsection'}):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # remove all tags with class 'metadata'\n",
    "    for tag in html.find_all('div', {'class': 'metadata'}):\n",
    "        tag.decompose()\n",
    "        \n",
    "    # remove all tags with class 'reflist'\n",
    "    for tag in html.find_all('div', {'class': 'reflist'}):\n",
    "        tag.decompose()\n",
    "        \n",
    "    # remove all links with class 'external text'\n",
    "    for tag in html.find_all('a', {'class': 'external text'}):\n",
    "        tag.decompose()\n",
    "        \n",
    "    # remove all map tags\n",
    "    for tag in html.find_all('map'):\n",
    "        tag.decompose()\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_1_clean = simplify_html(html_1)\n",
    "html_2_clean = simplify_html(html_2)\n",
    "text_1 = \"\\n\".join([line for line in html_1_clean.prettify().split('\\n') if line.strip() != ''])\n",
    "text_2 = \"\\n\".join([line for line in html_2_clean.prettify().split('\\n') if line.strip() != ''])\n",
    "sentences_1 = sent_tokenize(text_1)\n",
    "sentences_2 = sent_tokenize(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, if it is less than 50 characters, merge it with the next and previous sentences\n",
    "i = 0\n",
    "while i < len(sentences_1):\n",
    "    if len(sentences_1[i]) < 50:\n",
    "        if i > 0 and i < len(sentences_1) - 1:\n",
    "            sentences_1[i-1] = sentences_1[i-1] + ' ' + sentences_1[i] + ' ' + sentences_1[i+1]\n",
    "            del sentences_1[i]\n",
    "            del sentences_1[i]\n",
    "        elif i == 0:\n",
    "            sentences_1[i] = sentences_1[i] + ' ' + sentences_1[i+1]\n",
    "            del sentences_1[i+1]\n",
    "        else:\n",
    "            sentences_1[i-1] = sentences_1[i-1] + ' ' + sentences_1[i]\n",
    "            del sentences_1[i]\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_2):\n",
    "    if len(sentences_2[i]) < 50:\n",
    "        if i > 0 and i < len(sentences_2) - 1:\n",
    "            sentences_2[i-1] = sentences_2[i-1] + ' ' + sentences_2[i] + ' ' + sentences_2[i+1]\n",
    "            del sentences_2[i]\n",
    "            del sentences_2[i]\n",
    "        elif i == 0:\n",
    "            sentences_2[i] = sentences_2[i] + ' ' + sentences_2[i+1]\n",
    "            del sentences_2[i+1]\n",
    "        else:\n",
    "            sentences_2[i-1] = sentences_2[i-1] + ' ' + sentences_2[i]\n",
    "            del sentences_2[i]\n",
    "    else:\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont allow for links to be separated across sentences\n",
    "i = 0\n",
    "while i < len(sentences_1):\n",
    "    while sentences_1[i].count('<a ') > sentences_1[i].count('</a>'):\n",
    "        sentences_1[i] = sentences_1[i] + ' ' + sentences_1[i+1]\n",
    "        del sentences_1[i+1]\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_2):\n",
    "    while sentences_2[i].count('<a ') > sentences_2[i].count('</a>'):\n",
    "        sentences_2[i] = sentences_2[i] + ' ' + sentences_2[i+1]\n",
    "        del sentences_2[i+1]\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont allow parenthesis to be separated across sentences\n",
    "i = 0\n",
    "while i < len(sentences_1) - 1:\n",
    "    # find right-most occurrence of ')' and '(' in sentences_1[i]\n",
    "    right_paren_before = sentences_1[i].rfind(')')\n",
    "    left_paren_before = sentences_1[i].rfind('(')\n",
    "    right_paren_before = right_paren_before if right_paren_before != -1 else 0\n",
    "    left_paren_before = left_paren_before if left_paren_before != -1 else 0\n",
    "    # find left-most occurrence of ')' and '(' in sentences_1[i+1]\n",
    "    right_paren_after = sentences_1[i+1].find(')')\n",
    "    left_paren_after = sentences_1[i+1].find('(')\n",
    "    right_parent_after = right_paren_after if right_paren_after != -1 else len(sentences_1[i+1])\n",
    "    left_parent_after = left_paren_after if left_paren_after != -1 else len(sentences_1[i+1])\n",
    "\n",
    "    if right_paren_before < left_paren_before and right_parent_after < left_paren_after:\n",
    "        sentences_1[i] = sentences_1[i] + ' ' + sentences_1[i+1]\n",
    "        del sentences_1[i+1]\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_2) - 1:\n",
    "    # find right-most occurrence of ')' and '(' in sentences_2[i]\n",
    "    right_paren_before = sentences_2[i].rfind(')')\n",
    "    left_paren_before = sentences_2[i].rfind('(')\n",
    "    right_paren_before = right_paren_before if right_paren_before != -1 else 0\n",
    "    left_paren_before = left_paren_before if left_paren_before != -1 else 0\n",
    "    # find left-most occurrence of ')' and '(' in sentences_2[i+1]\n",
    "    right_paren_after = sentences_2[i+1].find(')')\n",
    "    left_paren_after = sentences_2[i+1].find('(')\n",
    "    right_parent_after = right_paren_after if right_paren_after != -1 else len(sentences_2[i+1])\n",
    "    left_parent_after = left_paren_after if left_paren_after != -1 else len(sentences_2[i+1])\n",
    "\n",
    "    if right_paren_before < left_paren_before and right_parent_after < left_paren_after:\n",
    "        sentences_2[i] = sentences_2[i] + ' ' + sentences_2[i+1]\n",
    "        del sentences_2[i+1]\n",
    "    else:\n",
    "        i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont allow list items to be separated across sentences\n",
    "i = 0\n",
    "while i < len(sentences_1) - 1:\n",
    "    while sentences_1[i].count('<li>') > sentences_1[i].count('</li>'):\n",
    "        sentences_1[i] = sentences_1[i] + ' ' + sentences_1[i+1]\n",
    "        del sentences_1[i+1]\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_2) - 1:\n",
    "    while sentences_2[i].count('<li>') > sentences_2[i].count('</li>'):\n",
    "        sentences_2[i] = sentences_2[i] + ' ' + sentences_2[i+1]\n",
    "        del sentences_2[i+1]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force </li> to act as a separator\n",
    "i = 0\n",
    "while i < len(sentences_1):\n",
    "    if '</li>' in sentences_1[i]:\n",
    "        extra_sentences = [s.strip() + '</li>' for s in sentences_1[i].split('</li>') if s.strip() != '']\n",
    "        del sentences_1[i]\n",
    "        sentences_1 = sentences_1[:i] + extra_sentences + sentences_1[i:]\n",
    "    i += 1\n",
    "    \n",
    "i = 0\n",
    "while i < len(sentences_2):\n",
    "    if '</li>' in sentences_2[i]:\n",
    "        extra_sentences = [s.strip() + '</li>' for s in sentences_2[i].split('</li>') if s.strip() != '']\n",
    "        del sentences_2[i]\n",
    "        sentences_2 = sentences_2[:i] + extra_sentences + sentences_2[i:]\n",
    "    i += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont allow h2 tags to be separated across sentences\n",
    "i = 0\n",
    "while i < len(sentences_1):\n",
    "    while sentences_1[i].count('<h2>') != sentences_1[i].count('</h2>'):\n",
    "        sentences_1[i] = sentences_1[i] + ' ' + sentences_1[i+1]\n",
    "        del sentences_1[i+1]\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_2):\n",
    "    while sentences_2[i].count('<h2>') != sentences_2[i].count('</h2>'):\n",
    "        sentences_2[i] = sentences_2[i] + ' ' + sentences_2[i+1]\n",
    "        del sentences_2[i+1]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force <h2> to act as a separator\n",
    "i = 0\n",
    "while i < len(sentences_1):\n",
    "    if '<h2>' in sentences_1[i]:\n",
    "        extra_sentences = [s.strip() for s in sentences_1[i].split('<h2>')]\n",
    "        extra_sentences[1:] = ['<h2>' + s for s in extra_sentences[1:]]\n",
    "        extra_sentences = [s for s in extra_sentences if s != '']\n",
    "        del sentences_1[i]\n",
    "        sentences_1 = sentences_1[:i] + extra_sentences + sentences_1[i:]\n",
    "    i += 1\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_2):\n",
    "    if '<h2>' in sentences_2[i]:\n",
    "        extra_sentences = [s.strip() for s in sentences_2[i].split('<h2>')]\n",
    "        extra_sentences[1:] = ['<h2>' + s for s in extra_sentences[1:]]\n",
    "        extra_sentences = [s for s in extra_sentences if s != '']\n",
    "        del sentences_2[i]\n",
    "        sentences_2 = sentences_2[:i] + extra_sentences + sentences_2[i:]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the differences between the two versions\n",
    "d = difflib.Differ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source_title)\n",
    "print(sample['target_title'].iloc[0])\n",
    "print(first_version)\n",
    "print(second_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match original and modified elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences = []\n",
    "new_sentences = []\n",
    "diff = d.compare(sentences_1, sentences_2)\n",
    "section_original = 'Lead'\n",
    "section_new = 'Lead'\n",
    "for line in diff:\n",
    "    if line.startswith('?'):\n",
    "        continue\n",
    "    elif line.startswith('+'):\n",
    "        soup = BeautifulSoup(line[2:].strip(), 'html.parser')\n",
    "        clean_text = soup.text.strip()\n",
    "        if clean_text.strip() == '':\n",
    "            continue\n",
    "        if '<h2>' in line:\n",
    "            h2 = soup.find('h2')\n",
    "            section_new = h2.text.strip()\n",
    "        new_sentences.append({'added': True, 'index': len(new_sentences), 'match': None, 'section': section_new, 'clean_sentence': clean_text, 'raw_sentence': line[2:].strip()})\n",
    "        words = new_sentences[-1]['clean_sentence'].split()\n",
    "        freqs = {}\n",
    "        for word in words:\n",
    "            freqs[word] = freqs.get(word, 0) + 1\n",
    "        norm = math.sqrt(sum([freqs[word] ** 2 for word in freqs]))\n",
    "        best_match = {'index': None, 'score': 0}\n",
    "        for i, sentence in enumerate(original_sentences):\n",
    "            if not sentence['removed']:\n",
    "                continue\n",
    "            # compute cossine similarity between all the sentence, and get the highest score\n",
    "            words = sentence['clean_sentence'].split()\n",
    "            freqs2 = {}\n",
    "            for word in words:\n",
    "                freqs2[word] = freqs2.get(word, 0) + 1\n",
    "            norm2 = math.sqrt(sum([freqs2[word] ** 2 for word in freqs2]))\n",
    "            score = 0\n",
    "            for word in freqs:\n",
    "                score += freqs[word] * freqs2.get(word, 0)\n",
    "            score = score / (norm * norm2)\n",
    "            if score > best_match['score']:\n",
    "                best_match['index'] = i\n",
    "                best_match['score'] = score\n",
    "        if best_match['score'] > 0.5:\n",
    "            original_sentences[best_match['index']]['match'] = new_sentences[-1]['index']\n",
    "            new_sentences[-1]['match'] = original_sentences[best_match['index']]['index']\n",
    "    elif line.startswith('-'):\n",
    "        soup = BeautifulSoup(line[2:].strip(), 'html.parser')\n",
    "        clean_text = soup.text.strip()\n",
    "        if clean_text.strip() == '':\n",
    "            continue\n",
    "        if '<h2>' in line:\n",
    "            h2 = soup.find('h2')\n",
    "            section_original = h2.text.strip()\n",
    "        original_sentences.append({'removed': True, 'index': len(original_sentences), 'match': None, 'section': section_original, 'clean_sentence': clean_text, 'raw_sentence': line[2:].strip()})\n",
    "    else:\n",
    "        soup = BeautifulSoup(line.strip(), 'html.parser')\n",
    "        clean_text = soup.text.strip()\n",
    "        if clean_text.strip() == '':\n",
    "            continue\n",
    "        if '<h2>' in line:\n",
    "            h2 = soup.find('h2')\n",
    "            section_original = h2.text.strip()\n",
    "            section_new = section_original\n",
    "        original_sentences.append({'removed': False, 'index': len(original_sentences), 'match': None, 'section': section_original, 'clean_sentence': clean_text, 'raw_sentence': line.strip()})\n",
    "        new_sentences.append({'added': False, 'index': len(new_sentences), 'match': None, 'section': section_new, 'clean_sentence': clean_text, 'raw_sentence': line.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in original_sentences:\n",
    "    if sentence['removed']:\n",
    "        print(sentence)\n",
    "\n",
    "for sentence in new_sentences:\n",
    "    if sentence['added']:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in new_sentences:\n",
    "    if sentence['match'] is not None:\n",
    "        print(sentence['raw_sentence'])\n",
    "        print(original_sentences[sentence['match']]['raw_sentence'])\n",
    "        print('##########')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_dump",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
